#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾é™ç»´è„šæœ¬
é€šè¿‡ç‰¹å¾é€‰æ‹©å’Œé™ç»´æŠ€æœ¯å‡å°‘å†…å­˜ä½¿ç”¨
"""

import pandas as pd
import numpy as np
import os
import sys
from datetime import datetime
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)


def reduce_features_by_importance(input_path: str, output_path: str, 
                                max_features: int = 50, method: str = 'random_forest'):
    """
    é€šè¿‡ç‰¹å¾é‡è¦æ€§å‡å°‘ç‰¹å¾æ•°é‡
    
    Args:
        input_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ•°æ®æ–‡ä»¶è·¯å¾„
        max_features: ä¿ç•™çš„æœ€å¤§ç‰¹å¾æ•°
        method: ç‰¹å¾é€‰æ‹©æ–¹æ³• ('random_forest', 'mutual_info', 'f_test')
    """
    print(f"ğŸ” å¼€å§‹ç‰¹å¾é™ç»´: {method}")
    print(f"   è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   ç›®æ ‡ç‰¹å¾æ•°: {max_features}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“ åŠ è½½æ•°æ®...")
    df = pd.read_parquet(input_path)
    print(f"   åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    feature_columns = [col for col in df.columns 
                      if col not in ['target', 'future_return', 'open_time', 'close_time']]
    
    X = df[feature_columns].values
    y = df['target'].values
    
    print(f"   åŸå§‹ç‰¹å¾æ•°: {len(feature_columns)}")
    print(f"   æ ·æœ¬æ•°: {len(X)}")
    
    # å¤„ç†NaNå€¼
    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
    
    # é€‰æ‹©ç‰¹å¾é€‰æ‹©æ–¹æ³•
    print(f"\nâš™ï¸  ä½¿ç”¨ {method} æ–¹æ³•é€‰æ‹©ç‰¹å¾...")
    
    if method == 'random_forest':
        # ä½¿ç”¨éšæœºæ£®æ—è¿›è¡Œç‰¹å¾é€‰æ‹©
        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=6)
        rf.fit(X, y)
        
        feature_importance = rf.feature_importances_
        feature_names = np.array(feature_columns)
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_indices = np.argsort(feature_importance)[::-1]
        selected_indices = sorted_indices[:max_features]
        selected_features = feature_names[selected_indices].tolist()
        
        print(f"   ğŸŒŸ Top 10 é‡è¦ç‰¹å¾:")
        for i in range(min(10, len(selected_indices))):
            idx = sorted_indices[i]
            print(f"      {i+1}. {feature_columns[idx]}: {feature_importance[idx]:.4f}")
    
    elif method == 'mutual_info':
        # ä½¿ç”¨äº’ä¿¡æ¯è¿›è¡Œç‰¹å¾é€‰æ‹©
        selector = SelectKBest(score_func=mutual_info_classif, k=max_features)
        X_selected = selector.fit_transform(X, y)
        
        selected_mask = selector.get_support()
        selected_features = [feature_columns[i] for i, selected in enumerate(selected_mask) if selected]
        
        scores = selector.scores_
        selected_scores = scores[selected_mask]
        
        print(f"   ğŸŒŸ Top 10 äº’ä¿¡æ¯å¾—åˆ†:")
        sorted_pairs = sorted(zip(selected_features, selected_scores), key=lambda x: x[1], reverse=True)
        for i, (feature, score) in enumerate(sorted_pairs[:10]):
            print(f"      {i+1}. {feature}: {score:.4f}")
    
    elif method == 'xgboost':
        # ä½¿ç”¨XGBoostè¿›è¡Œç‰¹å¾é€‰æ‹©
        print("   è®­ç»ƒXGBoostæ¨¡å‹...")
        
        # å¯¹æ ‡ç­¾è¿›è¡Œç¼–ç ï¼ˆXGBoostéœ€è¦è¿ç»­çš„æ ‡ç­¾ï¼‰
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)
        print(f"   åŸå§‹æ ‡ç­¾: {np.unique(y)} -> ç¼–ç åæ ‡ç­¾: {np.unique(y_encoded)}")
        
        # åˆ›å»ºXGBooståˆ†ç±»å™¨
        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=6,
            verbosity=0,  # å‡å°‘è¾“å‡º
            eval_metric='logloss'
        )
        
        # è®­ç»ƒæ¨¡å‹
        xgb_model.fit(X, y_encoded)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = xgb_model.feature_importances_
        feature_names = np.array(feature_columns)
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_indices = np.argsort(feature_importance)[::-1]
        selected_indices = sorted_indices[:max_features]
        selected_features = feature_names[selected_indices].tolist()
        
        print(f"   ğŸŒŸ Top 10 XGBoosté‡è¦ç‰¹å¾:")
        for i in range(min(10, len(selected_indices))):
            idx = sorted_indices[i]
            print(f"      {i+1}. {feature_columns[idx]}: {feature_importance[idx]:.4f}")
    
    elif method == 'f_test':
        # ä½¿ç”¨Fç»Ÿè®¡é‡è¿›è¡Œç‰¹å¾é€‰æ‹©
        selector = SelectKBest(score_func=f_classif, k=max_features)
        X_selected = selector.fit_transform(X, y)
        
        selected_mask = selector.get_support()
        selected_features = [feature_columns[i] for i, selected in enumerate(selected_mask) if selected]
        
        scores = selector.scores_
        selected_scores = scores[selected_mask]
        
        print(f"   ğŸŒŸ Top 10 Fç»Ÿè®¡é‡å¾—åˆ†:")
        sorted_pairs = sorted(zip(selected_features, selected_scores), key=lambda x: x[1], reverse=True)
        for i, (feature, score) in enumerate(sorted_pairs[:10]):
            print(f"      {i+1}. {feature}: {score:.4f}")
    
    # åˆ›å»ºé™ç»´åçš„æ•°æ®é›†
    print(f"\nğŸ“¦ åˆ›å»ºé™ç»´æ•°æ®é›†...")
    
    # ä¿ç•™çš„åˆ—ï¼šç›®æ ‡å˜é‡ + é€‰æ‹©çš„ç‰¹å¾
    keep_columns = ['target', 'future_return', 'open_time'] + selected_features
    reduced_df = df[keep_columns].copy()
    
    print(f"   é™ç»´åå½¢çŠ¶: {reduced_df.shape}")
    print(f"   ç‰¹å¾å‡å°‘: {len(feature_columns)} â†’ {len(selected_features)}")
    print(f"   å‹ç¼©æ¯”ä¾‹: {len(selected_features)/len(feature_columns)*100:.1f}%")
    
    # ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    reduced_df.to_parquet(output_path, index=False)
    
    # è®¡ç®—æ–‡ä»¶å¤§å°å¯¹æ¯”
    original_size = os.path.getsize(input_path) / (1024**2)  # MB
    reduced_size = os.path.getsize(output_path) / (1024**2)  # MB
    
    print(f"\nğŸ’¾ æ–‡ä»¶å¤§å°å¯¹æ¯”:")
    print(f"   åŸå§‹æ–‡ä»¶: {original_size:.1f} MB")
    print(f"   é™ç»´æ–‡ä»¶: {reduced_size:.1f} MB")
    print(f"   èŠ‚çœç©ºé—´: {(1 - reduced_size/original_size)*100:.1f}%")
    
    # ä¿å­˜ç‰¹å¾åˆ—è¡¨
    feature_list_path = output_path.replace('.parquet', '_features.txt')
    with open(feature_list_path, 'w') as f:
        f.write(f"# ç‰¹å¾é™ç»´ç»“æœ - {datetime.now()}\n")
        f.write(f"# æ–¹æ³•: {method}\n")
        f.write(f"# åŸå§‹ç‰¹å¾æ•°: {len(feature_columns)}\n")
        f.write(f"# é€‰æ‹©ç‰¹å¾æ•°: {len(selected_features)}\n\n")
        for feature in selected_features:
            f.write(f"{feature}\n")
    
    print(f"âœ… é™ç»´å®Œæˆ!")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   ç‰¹å¾åˆ—è¡¨: {feature_list_path}")
    
    return selected_features


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç‰¹å¾é™ç»´å·¥å…·')
    parser.add_argument('--input', type=str, 
                       default='processed_data/featured_data.parquet',
                       help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str,
                       default='processed_data/featured_data_reduced.parquet',
                       help='è¾“å‡ºæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max-features', type=int, default=50,
                       help='ä¿ç•™çš„æœ€å¤§ç‰¹å¾æ•° (é»˜è®¤: 50)')
    parser.add_argument('--method', type=str, default='random_forest',
                       choices=['random_forest', 'mutual_info', 'f_test', 'xgboost'],
                       help='ç‰¹å¾é€‰æ‹©æ–¹æ³•')
    
    args = parser.parse_args()
    
    print("ğŸ¯ ç‰¹å¾é™ç»´å·¥å…·")
    print("=" * 50)
    
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    try:
        selected_features = reduce_features_by_importance(
            input_path=args.input,
            output_path=args.output,
            max_features=args.max_features,
            method=args.method
        )
        
        print(f"\nğŸ‰ ç‰¹å¾é™ç»´æˆåŠŸ!")
        print(f"ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨å‡å°‘åçš„æ•°æ®è¿›è¡Œè®­ç»ƒ:")
        print(f"   æ•°æ®æ–‡ä»¶: {args.output}")
        print(f"   ç‰¹å¾æ•°é‡: {len(selected_features)}")
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾é™ç»´å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 