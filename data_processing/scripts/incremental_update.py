#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡æ›´æ–°è„šæœ¬
æ£€æŸ¥ç°æœ‰æ•°æ®æ˜¯å¦æœ€æ–°ï¼Œå¦‚æœä¸æ˜¯åˆ™è¿›è¡Œå¢é‡æ›´æ–°
"""

import argparse
import os
import sys
from datetime import datetime, timedelta
from typing import Optional
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from data_processing import DataPreprocessor
from database.connection import DatabaseConnection
from config.settings import load_project_config, get_legacy_config_dict


def check_data_freshness(raw_data_path: str, db_connection: DatabaseConnection, 
                         target_symbol: str) -> dict:
    """
    æ£€æŸ¥æ•°æ®æ–°é²œåº¦
    
    Args:
        raw_data_path: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
        db_connection: æ•°æ®åº“è¿æ¥
        target_symbol: ç›®æ ‡äº¤æ˜“å¯¹
        
    Returns:
        åŒ…å«æ£€æŸ¥ç»“æœçš„å­—å…¸
    """
    result = {
        'file_exists': False,
        'file_last_time': None,
        'db_last_time': None,
        'need_update': True,
        'new_records_count': 0
    }
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(raw_data_path):
            result['need_update'] = True
            result['new_records_count'] = 'unknown'
            return result
        
        result['file_exists'] = True
        
        # è·å–æ–‡ä»¶ä¸­çš„æœ€åæ—¶é—´
        import pandas as pd
        df = pd.read_parquet(raw_data_path)
        
        if 'open_time' in df.columns and len(df) > 0:
            result['file_last_time'] = int(df['open_time'].max())
            
            # è·å–æ•°æ®åº“ä¸­çš„æœ€åæ—¶é—´
            collection = db_connection.get_collection_for_symbol(target_symbol)
            latest_doc = collection.find().sort('open_time', -1).limit(1)
            latest_doc = list(latest_doc)
            
            if latest_doc:
                result['db_last_time'] = int(latest_doc[0]['open_time'])
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
                if result['db_last_time'] > result['file_last_time']:
                    result['need_update'] = True
                    
                    # è®¡ç®—æ–°è®°å½•æ•°
                    new_count = collection.count_documents({
                        'open_time': {'$gt': result['file_last_time']}
                    })
                    result['new_records_count'] = new_count
                else:
                    result['need_update'] = False
                    result['new_records_count'] = 0
            else:
                result['need_update'] = False
                
    except Exception as e:
        print(f"æ£€æŸ¥æ•°æ®æ–°é²œåº¦æ—¶å‡ºé”™: {e}")
        result['need_update'] = True
        result['error'] = str(e)
    
    return result


def format_timestamp(timestamp: Optional[int]) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    if timestamp is None:
        return "æœªçŸ¥"
    return datetime.fromtimestamp(timestamp / 1000).strftime('%Y-%m-%d %H:%M:%S')


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢é‡æ›´æ–°åŸå§‹æ•°æ®')
    parser.add_argument('--from-timestamp', type=int, 
                       help='æŒ‡å®šèµ·å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰')
    parser.add_argument('--from-date', type=str,
                       help='æŒ‡å®šèµ·å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰')
    parser.add_argument('--force-update', action='store_true',
                       help='å¼ºåˆ¶æ›´æ–°ï¼Œå³ä½¿æ•°æ®å·²ç»æ˜¯æœ€æ–°çš„')
    parser.add_argument('--config', type=str, default='config/config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/config.json)')
    parser.add_argument('--check-only', action='store_true',
                       help='ä»…æ£€æŸ¥æ•°æ®æ–°é²œåº¦ï¼Œä¸è¿›è¡Œæ›´æ–°')
    
    args = parser.parse_args()
    
    print("ğŸ”„ æ•°æ®å¢é‡æ›´æ–°å·¥å…·")
    print("=" * 50)
    print(f"âš™ï¸  é…ç½®æ–‡ä»¶: {args.config}")
    print(f"â° æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # åŠ è½½é…ç½®
        project_config = load_project_config(args.config)
        legacy_config = get_legacy_config_dict(project_config)
        target_symbol = project_config.data_collection.target_symbol
        
        print(f"ğŸ¯ ç›®æ ‡äº¤æ˜“å¯¹: {target_symbol}")
        print()
        
        # å»ºç«‹æ•°æ®åº“è¿æ¥
        db_connection = DatabaseConnection(legacy_config)
        
        # åˆå§‹åŒ–é¢„å¤„ç†å™¨
        preprocessor = DataPreprocessor(args.config)
        raw_data_path = preprocessor.get_raw_data_path()
        
        print(f"ğŸ“ åŸå§‹æ•°æ®æ–‡ä»¶: {raw_data_path}")
        
        # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
        print("\nğŸ” æ£€æŸ¥æ•°æ®æ–°é²œåº¦...")
        freshness = check_data_freshness(raw_data_path, db_connection, target_symbol)
        
        # æ˜¾ç¤ºæ£€æŸ¥ç»“æœ
        print(f"   æ–‡ä»¶å­˜åœ¨: {'æ˜¯' if freshness['file_exists'] else 'å¦'}")
        print(f"   æ–‡ä»¶æœ€åæ—¶é—´: {format_timestamp(freshness['file_last_time'])}")
        print(f"   æ•°æ®åº“æœ€åæ—¶é—´: {format_timestamp(freshness['db_last_time'])}")
        print(f"   éœ€è¦æ›´æ–°: {'æ˜¯' if freshness['need_update'] else 'å¦'}")
        print(f"   æ–°è®°å½•æ•°: {freshness['new_records_count']}")
        
        # å¦‚æœåªæ˜¯æ£€æŸ¥ï¼Œç›´æ¥è¿”å›
        if args.check_only:
            print("\nâœ… æ•°æ®æ–°é²œåº¦æ£€æŸ¥å®Œæˆ")
            return
        
        # ç¡®å®šèµ·å§‹æ—¶é—´
        last_processed_time = None
        if args.from_timestamp:
            last_processed_time = args.from_timestamp
            print(f"\nğŸ“… ä½¿ç”¨æŒ‡å®šèµ·å§‹æ—¶é—´æˆ³: {format_timestamp(last_processed_time)}")
        elif args.from_date:
            try:
                date_obj = datetime.strptime(args.from_date, '%Y-%m-%d')
                last_processed_time = int(date_obj.timestamp() * 1000)
                print(f"\nğŸ“… ä½¿ç”¨æŒ‡å®šèµ·å§‹æ—¥æœŸ: {format_timestamp(last_processed_time)}")
            except ValueError:
                print(f"âŒ æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {args.from_date} (åº”ä¸º YYYY-MM-DD)")
                sys.exit(1)
        else:
            last_processed_time = freshness.get('file_last_time')
            if last_processed_time:
                print(f"\nğŸ“… ä»æ–‡ä»¶æœ€åæ—¶é—´å¼€å§‹: {format_timestamp(last_processed_time)}")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
        if not args.force_update and not freshness['need_update']:
            print("\nâœ… æ•°æ®å·²ç»æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°")
            print("ğŸ’¡ å¦‚éœ€å¼ºåˆ¶æ›´æ–°ï¼Œè¯·ä½¿ç”¨ --force-update å‚æ•°")
            return
        
        if freshness['new_records_count'] == 0 and not args.force_update:
            print("\nâœ… æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")
            return
        
        # æ‰§è¡Œå¢é‡æ›´æ–°
        print(f"\nğŸš€ å¼€å§‹å¢é‡æ›´æ–°...")
        print(f"   é¢„æœŸæ–°å¢è®°å½•: {freshness['new_records_count']}")
        
        # æ›´æ–°åŸå§‹æ•°æ®
        start_time = datetime.now()
        new_data = preprocessor._load_incremental_data(
            target_symbol, 
            project_config.data_collection.feature_symbols,
            last_processed_time
        )
        
        if new_data.empty:
            print("âœ… æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†")
            return
        
        # æ›´æ–°raw_data.parquet
        if os.path.exists(raw_data_path):
            print("ğŸ“„ åˆå¹¶åˆ°ç°æœ‰æ–‡ä»¶...")
            existing_df = pd.read_parquet(raw_data_path)
            combined_df = pd.concat([existing_df, new_data], ignore_index=True)
            
            # å»é‡å¹¶æ’åº
            if 'open_time' in combined_df.columns:
                combined_df = combined_df.drop_duplicates(subset=['open_time'], keep='last')
                combined_df = combined_df.sort_values('open_time').reset_index(drop=True)
        else:
            print("ğŸ“„ åˆ›å»ºæ–°æ–‡ä»¶...")
            combined_df = new_data
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        combined_df.to_parquet(raw_data_path, index=False)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâœ… å¢é‡æ›´æ–°å®Œæˆï¼")
        print(f"   æ–°å¢æ•°æ®: {len(new_data)} æ¡")
        print(f"   æ€»æ•°æ®é‡: {len(combined_df)} æ¡")
        print(f"   å¤„ç†æ—¶é—´: {duration.total_seconds():.2f} ç§’")
        print(f"   æ›´æ–°æ–‡ä»¶: {raw_data_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¿›è¡Œç‰¹å¾å·¥ç¨‹
        featured_data_path = preprocessor.get_featured_data_path()
        if os.path.exists(featured_data_path):
            print(f"\nğŸ’¡ æç¤º: åŸå§‹æ•°æ®å·²æ›´æ–°ï¼Œå»ºè®®é‡æ–°è¿›è¡Œç‰¹å¾å·¥ç¨‹:")
            print(f"   uv run python3 data_processing/scripts/stage2_feature_engineering.py")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ­¢æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        # ç¡®ä¿å…³é—­æ•°æ®åº“è¿æ¥
        if 'db_connection' in locals():
            db_connection.close_all_connections()


if __name__ == "__main__":
    main() 