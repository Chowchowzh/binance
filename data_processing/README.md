# Data Processing æ•°æ®å¤„ç†æ¨¡å—

## ğŸ“‹ æ¦‚è¿°

`data_processing` æ¨¡å—æ˜¯å¸å®‰äº¤æ˜“ç­–ç•¥é¡¹ç›®çš„æ ¸å¿ƒæ•°æ®å¤„ç†å¼•æ“ï¼Œè´Ÿè´£ä»MongoDBæ•°æ®åº“ä¸­è·å–åŸå§‹Kçº¿æ•°æ®ï¼Œè¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œç”Ÿæˆæœºå™¨å­¦ä¹ å°±ç»ªçš„æ•°æ®é›†ã€‚

## ğŸ—ï¸ æ¨¡å—æ¶æ„

```
data_processing/
â”œâ”€â”€ preprocessor.py          # ä¸»è¦æ•°æ®é¢„å¤„ç†å™¨
â”œâ”€â”€ dataset_builder.py       # æ•°æ®é›†æ„å»ºå™¨
â”œâ”€â”€ torch_dataset.py         # PyTorchæ•°æ®é›†åŒ…è£…å™¨
â”œâ”€â”€ features/               # ç‰¹å¾å·¥ç¨‹å­æ¨¡å—
â”‚   â”œâ”€â”€ feature_builder.py     # ç‰¹å¾æ„å»ºå™¨
â”‚   â”œâ”€â”€ technical_indicators.py # æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
â”‚   â”œâ”€â”€ feature_utils.py       # ç‰¹å¾å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/                # æ‰§è¡Œè„šæœ¬ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ stage1_collect_data.py
â”‚   â”œâ”€â”€ stage2_feature_engineering.py
â”‚   â””â”€â”€ incremental_update.py
â””â”€â”€ README.md
```

## âš¡ æ ¸å¿ƒåŠŸèƒ½

### 1. ä¸¤é˜¶æ®µæ•°æ®å¤„ç†

- **é˜¶æ®µ1**: ä»MongoDBæ”¶é›†åŸå§‹Kçº¿æ•°æ® â†’ `processed_data/raw_data.parquet`
- **é˜¶æ®µ2**: ç‰¹å¾å·¥ç¨‹å’Œæ ‡å‡†åŒ– â†’ `processed_data/featured_data.parquet`

### 2. ç‰¹å¾å·¥ç¨‹

- æŠ€æœ¯æŒ‡æ ‡ï¼šRSI, MACD, å¸ƒæ—å¸¦, ATR ç­‰
- è®¢å•æµç‰¹å¾ï¼šOFI, ä¹°å–å‹åŠ›, ä»·æ ¼å½±å“
- å¸‚åœºå¾®è§‚ç»“æ„ï¼šæµåŠ¨æ€§, äº¤æ˜“é¢‘ç‡, VWAP
- è·¨èµ„äº§ç‰¹å¾ï¼šç›¸å…³æ€§, ä»·æ ¼æ¯”ç‡
- æ»åç‰¹å¾ï¼šä»·æ ¼å’Œæˆäº¤é‡çš„å†å²ä¿¡æ¯

### 3. æ•°æ®è´¨é‡ä¿è¯

- æ™ºèƒ½NaNå€¼å¤„ç†
- ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆä¿æŠ¤ç›®æ ‡å˜é‡å’Œä»·æ ¼æ•°æ®ï¼‰
- æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ’¡ å¿«é€Ÿæ¼”ç¤º

```bash
# æŸ¥çœ‹æ‰€æœ‰åŠŸèƒ½æ¼”ç¤º
uv run python3 data_processing/scripts/quick_demo.py

# æ£€æŸ¥å½“å‰æ•°æ®çŠ¶æ€
uv run python3 data_processing/scripts/incremental_update.py --check-only
```

### å®Œæ•´æµç¨‹ï¼ˆæ¨èï¼‰

```bash
# å®Œæ•´çš„ä¸¤é˜¶æ®µå¤„ç†
uv run python3 -c "
from data_processing import DataPreprocessor
processor = DataPreprocessor()
output_file = processor.process_data_two_stage(
    chunk_size=100000,
    normalize_features=True
)
print(f'å¤„ç†å®Œæˆï¼š{output_file}')
"
```

### åˆ†é˜¶æ®µæ‰§è¡Œ

#### ğŸ—„ï¸ é˜¶æ®µ1ï¼šæ”¶é›†åŸå§‹æ•°æ®

```bash
# æ–¹æ³•1ï¼šä½¿ç”¨è„šæœ¬
uv run python3 data_processing/scripts/stage1_collect_data.py

# æ–¹æ³•2ï¼šç›´æ¥è°ƒç”¨
uv run python3 -c "
from data_processing import DataPreprocessor
processor = DataPreprocessor()
raw_file = processor._stage1_collect_raw_data(
    chunk_size=100000,
    max_workers=None
)
print(f'åŸå§‹æ•°æ®æ”¶é›†å®Œæˆï¼š{raw_file}')
"
```

#### ğŸ”§ é˜¶æ®µ2ï¼šç‰¹å¾å·¥ç¨‹

```bash
# æ–¹æ³•1ï¼šä½¿ç”¨è„šæœ¬
uv run python3 data_processing/scripts/stage2_feature_engineering.py

# æ–¹æ³•2ï¼šç›´æ¥è°ƒç”¨
uv run python3 -c "
from data_processing import DataPreprocessor
processor = DataPreprocessor()
featured_file = processor._stage2_feature_engineering(
    'processed_data/raw_data.parquet',
    normalize_features=True
)
print(f'ç‰¹å¾å·¥ç¨‹å®Œæˆï¼š{featured_file}')
"
```

#### ğŸ”„ å¢é‡æ›´æ–°

```bash
# æ£€æŸ¥å¹¶å¢é‡æ›´æ–°åŸå§‹æ•°æ®
uv run python3 data_processing/scripts/incremental_update.py

# æˆ–è€…æŒ‡å®šèµ·å§‹æ—¶é—´
uv run python3 data_processing/scripts/incremental_update.py --from-timestamp 1640995200000
```

## ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯´æ˜

### åŸå§‹æ•°æ®æ–‡ä»¶ï¼š`processed_data/raw_data.parquet`

åŒ…å«å¤šä¸ªäº¤æ˜“å¯¹çš„OHLCVæ•°æ®ï¼š

```python
# åˆ—ç»“æ„ç¤ºä¾‹
[
    'open_time',           # å¼€ç›˜æ—¶é—´æˆ³
    'ETHUSDT_open',        # ETHå¼€ç›˜ä»·
    'ETHUSDT_high',        # ETHæœ€é«˜ä»·
    'ETHUSDT_low',         # ETHæœ€ä½ä»·
    'ETHUSDT_close',       # ETHæ”¶ç›˜ä»·
    'ETHUSDT_volume',      # ETHæˆäº¤é‡
    'BTCUSDT_open',        # BTCå¼€ç›˜ä»·
    # ... å…¶ä»–äº¤æ˜“å¯¹æ•°æ®
]
```

### ç‰¹å¾æ•°æ®æ–‡ä»¶ï¼š`processed_data/featured_data.parquet`

åŒ…å«201ä¸ªç‰¹å¾ + ç›®æ ‡å˜é‡ï¼š

```python
# ä¸»è¦ç‰¹å¾åˆ†ç±»
{
    'price_features': ['*_close', '*_high', '*_low', '*_open'],      # ä»·æ ¼ç‰¹å¾ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
    'volume_features': ['*_volume', '*_quote_asset_volume'],         # æˆäº¤é‡ç‰¹å¾ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
    'technical_indicators': ['*_rsi_*', '*_macd_*', '*_bb_*'],       # æŠ€æœ¯æŒ‡æ ‡ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
    'order_flow': ['*_ofi', '*_buy_pressure', '*_sell_pressure'],    # è®¢å•æµï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
    'lag_features': ['*_lag_*', '*_return_*'],                       # æ»åç‰¹å¾ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
    'cross_asset': ['*_corr_*', '*_ratio_*'],                        # è·¨èµ„äº§ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
    'target': 'target',           # ç›®æ ‡å˜é‡ï¼š-1(ä¸‹è·Œ), 0(æ¨ªç›˜), 1(ä¸Šæ¶¨)
    'future_return': 'future_return'  # æœªæ¥æ”¶ç›Šç‡ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
}
```

## ğŸ”§ é…ç½®é€‰é¡¹

### æ•°æ®å¤„ç†å‚æ•°

```python
# åœ¨ config/config.json ä¸­é…ç½®
{
    "data_collection": {
        "target_symbol": "ETHUSDT",                    # ä¸»è¦äº¤æ˜“å¯¹
        "feature_symbols": ["ETHUSDT", "BTCUSDT"],     # ç‰¹å¾äº¤æ˜“å¯¹
        "chunk_size": 100000,                          # æ•°æ®å—å¤§å°
        "max_workers": null                            # å¹¶å‘å·¥ä½œè¿›ç¨‹æ•°
    },
    "feature_engineering": {
        "normalize_features": true,                    # æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾
        "target_threshold": 0.0005,                    # ç›®æ ‡å˜é‡é˜ˆå€¼
        "clean_initial_nans": true                     # æ˜¯å¦æ¸…ç†åˆå§‹NaNå€¼
    }
}
```

### ç¯å¢ƒå˜é‡

```bash
# MongoDBè¿æ¥é…ç½®
export MONGODB_URI="mongodb://localhost:27017"
export MONGODB_DB_NAME="binance_data"
```

## ğŸ“ˆ ç›‘æ§å’Œè°ƒè¯•

### æ£€æŸ¥æ•°æ®è´¨é‡

```bash
# éªŒè¯ç›®æ ‡å˜é‡åˆ†å¸ƒ
uv run python3 -c "
import pandas as pd
df = pd.read_parquet('processed_data/featured_data.parquet')
print('ç›®æ ‡å˜é‡åˆ†å¸ƒ:')
print(df['target'].value_counts().sort_index())
print(f'æœªæ¥æ”¶ç›Šç‡ç»Ÿè®¡: å‡å€¼={df[\"future_return\"].mean():.6f}, æ ‡å‡†å·®={df[\"future_return\"].std():.6f}')
"
```

### æ£€æŸ¥æ•°æ®æ–°é²œåº¦

```bash
# æ£€æŸ¥æœ€æ–°æ•°æ®æ—¶é—´
uv run python3 -c "
import pandas as pd
from datetime import datetime
df = pd.read_parquet('processed_data/raw_data.parquet')
last_time = df['open_time'].max()
last_date = datetime.fromtimestamp(last_time/1000)
print(f'æœ€æ–°æ•°æ®æ—¶é—´: {last_date}')
"
```

## ğŸš¨ å¸¸è§é—®é¢˜

### 1. å†…å­˜ä¸è¶³

```bash
# å‡å°‘chunk_size
uv run python3 -c "
from data_processing import DataPreprocessor
processor = DataPreprocessor()
processor.process_data_two_stage(chunk_size=50000)  # å‡å°‘åˆ°5ä¸‡
"
```

### 2. æ•°æ®åº“è¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥MongoDBè¿æ¥
uv run python3 -c "
from database.connection import DatabaseConnection
from config.settings import load_project_config, get_legacy_config_dict
config = load_project_config()
legacy_config = get_legacy_config_dict(config)
db = DatabaseConnection(legacy_config)
print('æ•°æ®åº“è¿æ¥æˆåŠŸ')
"
```

### 3. ç›®æ ‡å˜é‡åˆ†å¸ƒå¼‚å¸¸

- æ£€æŸ¥é˜ˆå€¼è®¾ç½®æ˜¯å¦åˆç†ï¼ˆæ¨è0.0005ï¼‰
- ç¡®è®¤ä»·æ ¼æ•°æ®æœªè¢«é”™è¯¯æ ‡å‡†åŒ–
- éªŒè¯æœªæ¥æ”¶ç›Šç‡çš„è®¡ç®—é€»è¾‘

## ğŸ¯ å®é™…ä½¿ç”¨ç¤ºä¾‹

### æ—¥å¸¸æ•°æ®ç»´æŠ¤

```bash
# 1. æ£€æŸ¥æ•°æ®æ–°é²œåº¦
uv run python3 data_processing/scripts/incremental_update.py --check-only

# 2. å¦‚æœæœ‰æ–°æ•°æ®ï¼Œè¿›è¡Œå¢é‡æ›´æ–°
uv run python3 data_processing/scripts/incremental_update.py

# 3. é‡æ–°ç”Ÿæˆç‰¹å¾æ•°æ®
uv run python3 data_processing/scripts/stage2_feature_engineering.py
```

### ä»é›¶å¼€å§‹å»ºç«‹æ•°æ®é›†

```bash
# 1. æ”¶é›†æ‰€æœ‰åŸå§‹æ•°æ®
uv run python3 data_processing/scripts/stage1_collect_data.py --chunk-size 100000

# 2. è¿›è¡Œç‰¹å¾å·¥ç¨‹
uv run python3 data_processing/scripts/stage2_feature_engineering.py

# 3. éªŒè¯æ•°æ®è´¨é‡
uv run python3 -c "
import pandas as pd
df = pd.read_parquet('processed_data/featured_data.parquet')
print(f'æ•°æ®å½¢çŠ¶: {df.shape}')
print('ç›®æ ‡å˜é‡åˆ†å¸ƒ:')
print(df['target'].value_counts().sort_index())
"
```

### å†…å­˜ä¼˜åŒ–çš„å¤§æ•°æ®å¤„ç†

```bash
# å¯¹äºè¶…å¤§æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
uv run python3 data_processing/scripts/stage1_collect_data.py \
  --chunk-size 50000 \
  --max-workers 2

# è·³è¿‡ç‰¹å¾æ ‡å‡†åŒ–ä»¥èŠ‚çœå†…å­˜
uv run python3 data_processing/scripts/stage2_feature_engineering.py \
  --no-normalize
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [ç­–ç•¥æŒ‡å—](../strategy/STRATEGY_GUIDE.md)
- [æ¨¡å‹è®­ç»ƒ](../strategy/training/)
- [å›æµ‹åˆ†æ](../strategy/backtesting/)

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ—¥å¿—è¾“å‡ºä¸­çš„é”™è¯¯ä¿¡æ¯
2. æ•°æ®åº“è¿æ¥çŠ¶æ€
3. ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³
4. Pythonç¯å¢ƒå’Œä¾èµ–åŒ…ç‰ˆæœ¬

### å¸¸ç”¨è°ƒè¯•å‘½ä»¤

```bash
# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
uv run python3 data_processing/scripts/incremental_update.py --help

# å¿«é€ŸåŠŸèƒ½æ¼”ç¤º
uv run python3 data_processing/scripts/quick_demo.py

# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
uv run python3 -c "
from data_processing import DataPreprocessor
processor = DataPreprocessor()
info = processor.validate_output_data()
print(info)
"
``` 