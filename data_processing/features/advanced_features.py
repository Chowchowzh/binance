# -*- coding: utf-8 -*-
"""
é«˜çº§ç‰¹å¾å·¥ç¨‹æ¨¡å—
åŒ…å«æ»šåŠ¨æ³¢åŠ¨ç‡ã€å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾ã€é«˜çº§ç»Ÿè®¡ç‰¹å¾ç­‰
åŸºäºé‡‘èæœºå™¨å­¦ä¹ æœ€ä½³å®è·µæ„å»ºé«˜è´¨é‡ç‰¹å¾
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
import warnings
from numba import jit
from scipy import stats
from scipy.signal import savgol_filter
from sklearn.preprocessing import RobustScaler
try:
    import talib
    HAS_TALIB = True
except ImportError:
    HAS_TALIB = False
    print("Warning: TA-Lib not available. Technical features will be skipped.")

warnings.filterwarnings('ignore')


class AdvancedFeatureEngineer:
    """
    é«˜çº§ç‰¹å¾å·¥ç¨‹å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ»šåŠ¨æ³¢åŠ¨ç‡ä¼°è®¡å™¨ï¼ˆå¤šç§æ–¹æ³•ï¼‰
    2. å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾
    3. é«˜çº§ç»Ÿè®¡ç‰¹å¾
    4. åˆ†æ•°åŒ–å·®åˆ†ç‰¹å¾
    5. è¶‹åŠ¿å’Œå‘¨æœŸåˆ†è§£ç‰¹å¾
    """
    
    def __init__(self, 
                 volatility_windows: List[int] = [5, 10, 20, 50],
                 microstructure_windows: List[int] = [5, 15, 30],
                 statistical_windows: List[int] = [10, 20, 50, 100]):
        """
        åˆå§‹åŒ–é«˜çº§ç‰¹å¾å·¥ç¨‹å™¨
        
        Args:
            volatility_windows: æ³¢åŠ¨ç‡è®¡ç®—çª—å£åˆ—è¡¨
            microstructure_windows: å¾®è§‚ç»“æ„ç‰¹å¾çª—å£åˆ—è¡¨  
            statistical_windows: ç»Ÿè®¡ç‰¹å¾çª—å£åˆ—è¡¨
        """
        self.volatility_windows = volatility_windows
        self.microstructure_windows = microstructure_windows
        self.statistical_windows = statistical_windows
        
        print(f"åˆå§‹åŒ–é«˜çº§ç‰¹å¾å·¥ç¨‹å™¨:")
        print(f"  - æ³¢åŠ¨ç‡çª—å£: {volatility_windows}")
        print(f"  - å¾®è§‚ç»“æ„çª—å£: {microstructure_windows}")
        print(f"  - ç»Ÿè®¡ç‰¹å¾çª—å£: {statistical_windows}")
    
    def build_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„å»ºå¤šç§æ³¢åŠ¨ç‡ç‰¹å¾
        
        Args:
            df: OHLCVæ•°æ®DataFrame
            
        Returns:
            åŒ…å«æ³¢åŠ¨ç‡ç‰¹å¾çš„DataFrame
        """
        print("æ„å»ºæ³¢åŠ¨ç‡ç‰¹å¾...")
        
        features_df = df.copy()
        
        for window in self.volatility_windows:
            # 1. ç®€å•æ³¢åŠ¨ç‡ï¼ˆå¯¹æ•°æ”¶ç›Šç‡æ ‡å‡†å·®ï¼‰
            log_returns = np.log(df['close'] / df['close'].shift(1))
            features_df[f'volatility_simple_{window}'] = log_returns.rolling(window).std()
            
            # 2. Garman-Klassæ³¢åŠ¨ç‡ä¼°è®¡å™¨
            features_df[f'volatility_gk_{window}'] = self._garman_klass_volatility(
                df['high'], df['low'], df['open'], df['close'], window
            )
            
            # 3. Rogers-Satchellæ³¢åŠ¨ç‡ä¼°è®¡å™¨
            features_df[f'volatility_rs_{window}'] = self._rogers_satchell_volatility(
                df['high'], df['low'], df['open'], df['close'], window
            )
            
            # 4. Yang-Zhangæ³¢åŠ¨ç‡ä¼°è®¡å™¨
            features_df[f'volatility_yz_{window}'] = self._yang_zhang_volatility(
                df['high'], df['low'], df['open'], df['close'], window
            )
            
            # 5. Parkinsonæ³¢åŠ¨ç‡ï¼ˆé«˜ä½ä»·å·®ï¼‰
            features_df[f'volatility_parkinson_{window}'] = self._parkinson_volatility(
                df['high'], df['low'], window
            )
            
            # 6. å®ç°æ³¢åŠ¨ç‡çš„åˆ†ä½æ•°
            rv = log_returns.abs().rolling(window).sum()
            features_df[f'realized_vol_rank_{window}'] = rv.rolling(window*2).rank(pct=True)
            
            # 7. æ³¢åŠ¨ç‡çš„æ³¢åŠ¨ç‡
            vol_simple = features_df[f'volatility_simple_{window}']
            features_df[f'vol_of_vol_{window}'] = vol_simple.rolling(window).std()
        
        print(f"  - ç”Ÿæˆæ³¢åŠ¨ç‡ç‰¹å¾: {len([c for c in features_df.columns if 'volatility' in c])} ä¸ª")
        
        return features_df
    
    def build_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„å»ºå¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾
        
        Args:
            df: OHLCVæ•°æ®DataFrame
            
        Returns:
            åŒ…å«å¾®è§‚ç»“æ„ç‰¹å¾çš„DataFrame
        """
        print("æ„å»ºå¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾...")
        
        features_df = df.copy()
        
        for window in self.microstructure_windows:
            # 1. æˆäº¤é‡ç›¸å…³ç‰¹å¾
            features_df[f'volume_sma_{window}'] = df['volume'].rolling(window).mean()
            features_df[f'volume_std_{window}'] = df['volume'].rolling(window).std()
            features_df[f'volume_rank_{window}'] = df['volume'].rolling(window*2).rank(pct=True)
            
            # 2. ä»·æ ¼-æˆäº¤é‡å…³ç³»
            price_change = df['close'].pct_change()
            volume_change = df['volume'].pct_change()
            features_df[f'price_volume_corr_{window}'] = price_change.rolling(window).corr(volume_change)
            
            # 3. VWAPç‰¹å¾
            vwap = (df['close'] * df['volume']).rolling(window).sum() / df['volume'].rolling(window).sum()
            features_df[f'vwap_{window}'] = vwap
            features_df[f'price_to_vwap_{window}'] = df['close'] / vwap
            
            # 4. ä¹°å–å‹åŠ›æŒ‡æ ‡ï¼ˆåŸºäºå½±çº¿ï¼‰
            body_size = abs(df['close'] - df['open'])
            upper_shadow = df['high'] - np.maximum(df['open'], df['close'])
            lower_shadow = np.minimum(df['open'], df['close']) - df['low']
            
            features_df[f'upper_shadow_ratio_{window}'] = (upper_shadow / body_size).rolling(window).mean()
            features_df[f'lower_shadow_ratio_{window}'] = (lower_shadow / body_size).rolling(window).mean()
            
            # 5. æµåŠ¨æ€§ä»£ç†æŒ‡æ ‡
            high_low_spread = (df['high'] - df['low']) / df['close']
            features_df[f'liquidity_proxy_{window}'] = high_low_spread.rolling(window).mean()
            
            # 6. ä»·æ ¼è·³è·ƒæ£€æµ‹
            log_returns = np.log(df['close'] / df['close'].shift(1))
            jump_threshold = log_returns.rolling(window*2).std() * 3
            features_df[f'price_jump_{window}'] = (log_returns.abs() > jump_threshold).rolling(window).sum()
            
            # 7. è®¢å•æµä¸å¹³è¡¡ä»£ç†ï¼ˆåŸºäºæ”¶ç›˜ä»·ç›¸å¯¹ä½ç½®ï¼‰
            close_position = (df['close'] - df['low']) / (df['high'] - df['low'])
            features_df[f'order_flow_imbalance_{window}'] = close_position.rolling(window).mean()
        
        print(f"  - ç”Ÿæˆå¾®è§‚ç»“æ„ç‰¹å¾: {len([c for c in features_df.columns if any(x in c for x in ['volume', 'vwap', 'shadow', 'liquidity', 'jump', 'order_flow'])])} ä¸ª")
        
        return features_df
    
    def build_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„å»ºé«˜çº§ç»Ÿè®¡ç‰¹å¾
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            åŒ…å«ç»Ÿè®¡ç‰¹å¾çš„DataFrame
        """
        print("æ„å»ºé«˜çº§ç»Ÿè®¡ç‰¹å¾...")
        
        features_df = df.copy()
        log_returns = np.log(df['close'] / df['close'].shift(1))
        
        for window in self.statistical_windows:
            # 1. é«˜é˜¶çŸ©ç‰¹å¾
            features_df[f'skewness_{window}'] = log_returns.rolling(window).skew()
            features_df[f'kurtosis_{window}'] = log_returns.rolling(window).kurt()
            
            # 2. åˆ†ä½æ•°ç‰¹å¾
            features_df[f'quantile_25_{window}'] = log_returns.rolling(window).quantile(0.25)
            features_df[f'quantile_75_{window}'] = log_returns.rolling(window).quantile(0.75)
            features_df[f'iqr_{window}'] = (features_df[f'quantile_75_{window}'] - 
                                           features_df[f'quantile_25_{window}'])
            
            # 3. å°¾éƒ¨é£é™©ç‰¹å¾
            features_df[f'var_5_{window}'] = log_returns.rolling(window).quantile(0.05)
            features_df[f'cvar_5_{window}'] = log_returns[log_returns <= features_df[f'var_5_{window}']].rolling(window).mean()
            
            # 4. è‡ªç›¸å…³ç‰¹å¾
            features_df[f'autocorr_1_{window}'] = log_returns.rolling(window).apply(lambda x: x.autocorr(lag=1))
            features_df[f'autocorr_5_{window}'] = log_returns.rolling(window).apply(lambda x: x.autocorr(lag=5))
            
            # 5. è¶‹åŠ¿å¼ºåº¦
            price_sma = df['close'].rolling(window).mean()
            features_df[f'trend_strength_{window}'] = (df['close'] - price_sma) / price_sma
            
            # 6. åŠ¨é‡ç‰¹å¾
            features_df[f'momentum_{window}'] = df['close'] / df['close'].shift(window) - 1
            
            # 7. å‡å€¼å›å½’ç‰¹å¾
            z_score = (df['close'] - price_sma) / df['close'].rolling(window).std()
            features_df[f'mean_reversion_{window}'] = z_score
            
            # 8. æ³¢åŠ¨ç‡èšé›†æ€§
            vol_cluster = log_returns.abs().rolling(window).mean()
            features_df[f'volatility_clustering_{window}'] = vol_cluster
            
            # 9. ä¿¡æ¯æ¯”ç‡
            if window >= 20:
                excess_return = log_returns - log_returns.rolling(window*2).mean()
                tracking_error = excess_return.rolling(window).std()
                features_df[f'information_ratio_{window}'] = excess_return.rolling(window).mean() / tracking_error
        
        print(f"  - ç”Ÿæˆç»Ÿè®¡ç‰¹å¾: {len([c for c in features_df.columns if any(x in c for x in ['skewness', 'kurtosis', 'quantile', 'var_', 'autocorr', 'trend', 'momentum', 'mean_reversion', 'clustering', 'information'])])} ä¸ª")
        
        return features_df
    
    def build_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„å»ºæŠ€æœ¯åˆ†æç‰¹å¾
        
        Args:
            df: OHLCVæ•°æ®DataFrame
            
        Returns:
            åŒ…å«æŠ€æœ¯ç‰¹å¾çš„DataFrame
        """
        print("æ„å»ºæŠ€æœ¯åˆ†æç‰¹å¾...")
        
        features_df = df.copy()
        
        if not HAS_TALIB:
            print("  - è·³è¿‡æŠ€æœ¯åˆ†æç‰¹å¾ï¼ˆTA-Libä¸å¯ç”¨ï¼‰")
            return features_df
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        high = df['high'].astype(float).values
        low = df['low'].astype(float).values
        close = df['close'].astype(float).values
        volume = df['volume'].astype(float).values
        
        # 1. è¶‹åŠ¿æŒ‡æ ‡
        features_df['adx_14'] = talib.ADX(high, low, close, timeperiod=14)
        features_df['cci_14'] = talib.CCI(high, low, close, timeperiod=14)
        features_df['dx_14'] = talib.DX(high, low, close, timeperiod=14)
        
        # 2. åŠ¨é‡æŒ‡æ ‡
        features_df['rsi_14'] = talib.RSI(close, timeperiod=14)
        features_df['roc_10'] = talib.ROC(close, timeperiod=10)
        features_df['mom_10'] = talib.MOM(close, timeperiod=10)
        
        # 3. æ³¢åŠ¨æ€§æŒ‡æ ‡
        features_df['atr_14'] = talib.ATR(high, low, close, timeperiod=14)
        features_df['natr_14'] = talib.NATR(high, low, close, timeperiod=14)
        
        # 4. æˆäº¤é‡æŒ‡æ ‡
        features_df['ad'] = talib.AD(high, low, close, volume)
        features_df['adosc'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)
        features_df['obv'] = talib.OBV(close, volume)
        
        # 5. ä»·æ ¼ä½ç½®æŒ‡æ ‡
        features_df['willr_14'] = talib.WILLR(high, low, close, timeperiod=14)
        
        # 6. å¸ƒæ—å¸¦
        bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)
        features_df['bb_upper'] = bb_upper
        features_df['bb_middle'] = bb_middle
        features_df['bb_lower'] = bb_lower
        features_df['bb_width'] = (bb_upper - bb_lower) / bb_middle
        features_df['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower)
        
        # 7. MACD
        macd, macd_signal, macd_hist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
        features_df['macd'] = macd
        features_df['macd_signal'] = macd_signal
        features_df['macd_hist'] = macd_hist
        
        print(f"  - ç”ŸæˆæŠ€æœ¯åˆ†æç‰¹å¾: {len([c for c in features_df.columns if c not in df.columns])} ä¸ª")
        
        return features_df
    
    def build_fractal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„å»ºåˆ†å½¢å’Œåˆ†æ•°åŒ–å·®åˆ†ç‰¹å¾
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            åŒ…å«åˆ†å½¢ç‰¹å¾çš„DataFrame
        """
        print("æ„å»ºåˆ†å½¢ç‰¹å¾...")
        
        features_df = df.copy()
        log_prices = np.log(df['close'])
        
        # 1. åˆ†æ•°åŒ–å·®åˆ†ï¼ˆä¸åŒé˜¶æ•°ï¼‰
        for d in [0.1, 0.2, 0.3, 0.4, 0.5]:
            features_df[f'frac_diff_{int(d*10)}'] = self._fractional_diff(log_prices, d)
        
        # 2. HurstæŒ‡æ•°ï¼ˆä¸åŒçª—å£ï¼‰
        for window in [50, 100, 200]:
            if len(df) >= window:
                features_df[f'hurst_{window}'] = log_prices.rolling(window).apply(
                    lambda x: self._hurst_exponent(x.values) if len(x.dropna()) >= 20 else np.nan
                )
        
        # 3. å»è¶‹åŠ¿æ³¢åŠ¨åˆ†æï¼ˆDFAï¼‰
        for window in [50, 100]:
            if len(df) >= window:
                features_df[f'dfa_{window}'] = log_prices.rolling(window).apply(
                    lambda x: self._detrended_fluctuation_analysis(x.values) if len(x.dropna()) >= 20 else np.nan
                )
        
        print(f"  - ç”Ÿæˆåˆ†å½¢ç‰¹å¾: {len([c for c in features_df.columns if any(x in c for x in ['frac_diff', 'hurst', 'dfa'])])} ä¸ª")
        
        return features_df
    
    def build_all_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ„å»ºæ‰€æœ‰é«˜çº§ç‰¹å¾
        
        Args:
            df: OHLCVæ•°æ®DataFrame
            
        Returns:
            åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrame
        """
        print("\n" + "="*80)
        print("ğŸ”§ å¼€å§‹æ„å»ºæ‰€æœ‰é«˜çº§ç‰¹å¾")
        print("="*80)
        
        start_time = pd.Timestamp.now()
        
        # æŒ‰é¡ºåºæ„å»ºå„ç±»ç‰¹å¾
        features_df = self.build_volatility_features(df)
        features_df = self.build_microstructure_features(features_df)
        features_df = self.build_statistical_features(features_df)
        features_df = self.build_technical_features(features_df)
        features_df = self.build_fractal_features(features_df)
        
        # ç‰¹å¾æ¸…ç†
        features_df = self._clean_features(features_df)
        
        end_time = pd.Timestamp.now()
        elapsed = (end_time - start_time).total_seconds()
        
        original_features = len(df.columns)
        new_features = len(features_df.columns) - original_features
        
        print(f"\nâœ… ç‰¹å¾æ„å»ºå®Œæˆ:")
        print(f"  - åŸå§‹ç‰¹å¾: {original_features}")
        print(f"  - æ–°å¢ç‰¹å¾: {new_features}")
        print(f"  - æ€»ç‰¹å¾æ•°: {len(features_df.columns)}")
        print(f"  - ç”¨æ—¶: {elapsed:.2f}ç§’")
        
        return features_df
    
    def _clean_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†ç‰¹å¾æ•°æ®"""
        print("æ¸…ç†ç‰¹å¾æ•°æ®...")
        
        # å¤„ç†æ— ç©·å€¼
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # è®¡ç®—ç‰¹å¾è´¨é‡ç»Ÿè®¡
        nan_ratios = df.isnull().sum() / len(df)
        high_nan_features = nan_ratios[nan_ratios > 0.5].index.tolist()
        
        if high_nan_features:
            print(f"  - ç§»é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾: {len(high_nan_features)} ä¸ª")
            df = df.drop(columns=high_nan_features)
        
        # å‰å‘å¡«å……ç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        print(f"  - æœ€ç»ˆç‰¹å¾æ•°: {len(df.columns)}")
        
        return df
    
    # === å†…éƒ¨è¾…åŠ©æ–¹æ³• ===
    
    def _garman_klass_volatility(self, high, low, open_price, close, window):
        """Garman-Klassæ³¢åŠ¨ç‡ä¼°è®¡å™¨"""
        hl = np.log(high / low)
        co = np.log(close / open_price)
        gk = 0.5 * hl**2 - (2*np.log(2)-1) * co**2
        return gk.rolling(window).mean().apply(np.sqrt)
    
    def _rogers_satchell_volatility(self, high, low, open_price, close, window):
        """Rogers-Satchellæ³¢åŠ¨ç‡ä¼°è®¡å™¨"""
        ho = np.log(high / open_price)
        hc = np.log(high / close)
        lo = np.log(low / open_price)
        lc = np.log(low / close)
        rs = ho * hc + lo * lc
        return rs.rolling(window).mean().apply(np.sqrt)
    
    def _yang_zhang_volatility(self, high, low, open_price, close, window):
        """Yang-Zhangæ³¢åŠ¨ç‡ä¼°è®¡å™¨"""
        log_ho = np.log(high / open_price)
        log_lo = np.log(low / open_price)
        log_co = np.log(close / open_price)
        log_oc = np.log(open_price / close.shift(1))
        log_cc = np.log(close / close.shift(1))
        
        rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)
        close_to_close = log_cc**2
        open_to_close = log_oc**2
        
        k = 0.34 / (1.34 + (window+1)/(window-1))
        yz = open_to_close + k*close_to_close + (1-k)*rs
        
        return yz.rolling(window).mean().apply(np.sqrt)
    
    def _parkinson_volatility(self, high, low, window):
        """Parkinsonæ³¢åŠ¨ç‡ä¼°è®¡å™¨"""
        hl = np.log(high / low)
        parkinson = hl**2 / (4 * np.log(2))
        return parkinson.rolling(window).mean().apply(np.sqrt)
    
    def _fractional_diff(self, series: pd.Series, d: float) -> pd.Series:
        """è®¡ç®—åˆ†æ•°åŒ–å·®åˆ†"""
        try:
            from statsmodels.tsa.stattools import adfuller
            from sklearn.linear_model import LinearRegression
            
            # ç®€åŒ–å®ç°ï¼šä½¿ç”¨å·®åˆ†è¿‘ä¼¼
            if d == 0:
                return series
            elif d == 1:
                return series.diff()
            else:
                # ä½¿ç”¨çº¿æ€§æ’å€¼è¿‘ä¼¼åˆ†æ•°åŒ–å·®åˆ†
                diff1 = series.diff()
                diff0 = series
                return diff0 * (1 - d) + diff1 * d
        except:
            return series.diff() * d
    
    def _hurst_exponent(self, ts: np.ndarray) -> float:
        """è®¡ç®—HurstæŒ‡æ•°"""
        try:
            lags = range(2, min(20, len(ts)//4))
            tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]
            poly = np.polyfit(np.log(lags), np.log(tau), 1)
            return poly[0] * 2.0
        except:
            return 0.5  # éšæœºæ¸¸èµ°çš„ç†è®ºå€¼
    
    def _detrended_fluctuation_analysis(self, ts: np.ndarray) -> float:
        """å»è¶‹åŠ¿æ³¢åŠ¨åˆ†æ"""
        try:
            N = len(ts)
            if N < 20:
                return np.nan
            
            # ç§¯åˆ†åºåˆ—
            y = np.cumsum(ts - np.mean(ts))
            
            # ä¸åŒçª—å£å¤§å°
            scales = np.logspace(1, np.log10(N//4), 10).astype(int)
            fluctuations = []
            
            for scale in scales:
                # åˆ†æ®µ
                segments = N // scale
                variance = []
                
                for i in range(segments):
                    start, end = i * scale, (i + 1) * scale
                    segment = y[start:end]
                    
                    # å»è¶‹åŠ¿
                    t = np.arange(len(segment))
                    poly = np.polyfit(t, segment, 1)
                    trend = np.polyval(poly, t)
                    
                    variance.append(np.mean((segment - trend) ** 2))
                
                fluctuations.append(np.sqrt(np.mean(variance)))
            
            # è®¡ç®—ç¼©æ”¾æŒ‡æ•°
            coeffs = np.polyfit(np.log(scales), np.log(fluctuations), 1)
            return coeffs[0]
        except:
            return 0.5


# ä¾¿åˆ©å‡½æ•°
def add_advanced_features(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """
    ä¸ºDataFrameæ·»åŠ é«˜çº§ç‰¹å¾
    
    Args:
        df: OHLCVæ•°æ®DataFrame
        **kwargs: AdvancedFeatureEngineerçš„å‚æ•°
        
    Returns:
        åŒ…å«é«˜çº§ç‰¹å¾çš„DataFrame
    """
    engineer = AdvancedFeatureEngineer(**kwargs)
    return engineer.build_all_features(df)


def analyze_feature_importance(df: pd.DataFrame, 
                              target_column: str,
                              feature_columns: List[str] = None) -> pd.DataFrame:
    """
    åˆ†æç‰¹å¾é‡è¦æ€§
    
    Args:
        df: åŒ…å«ç‰¹å¾å’Œç›®æ ‡çš„DataFrame
        target_column: ç›®æ ‡åˆ—å
        feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
        
    Returns:
        ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ
    """
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.feature_selection import mutual_info_regression
    from scipy.stats import pearsonr
    
    if feature_columns is None:
        feature_columns = [col for col in df.columns if col != target_column]
    
    # å‡†å¤‡æ•°æ®
    X = df[feature_columns].fillna(0)
    y = df[target_column].fillna(0)
    
    # æœ‰æ•ˆæ ·æœ¬
    valid_mask = ~(np.isnan(y) | np.isinf(y))
    X = X[valid_mask]
    y = y[valid_mask]
    
    if len(X) == 0:
        return pd.DataFrame()
    
    # éšæœºæ£®æ—é‡è¦æ€§
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    rf_importance = rf.feature_importances_
    
    # äº’ä¿¡æ¯
    try:
        mi_scores = mutual_info_regression(X, y, random_state=42)
    except:
        mi_scores = np.zeros(len(feature_columns))
    
    # ç›¸å…³ç³»æ•°
    correlations = []
    for col in feature_columns:
        try:
            corr, _ = pearsonr(X[col], y)
            correlations.append(abs(corr) if not np.isnan(corr) else 0)
        except:
            correlations.append(0)
    
    # æ±‡æ€»ç»“æœ
    importance_df = pd.DataFrame({
        'feature': feature_columns,
        'rf_importance': rf_importance,
        'mutual_info': mi_scores,
        'correlation': correlations
    })
    
    # ç»¼åˆè¯„åˆ†
    importance_df['composite_score'] = (
        0.5 * importance_df['rf_importance'] + 
        0.3 * importance_df['mutual_info'] + 
        0.2 * importance_df['correlation']
    )
    
    return importance_df.sort_values('composite_score', ascending=False) 