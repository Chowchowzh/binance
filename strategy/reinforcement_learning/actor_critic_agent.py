#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Actor-Criticå¼ºåŒ–å­¦ä¹ Agent - åŸºäºPPOç®—æ³•çš„äº¤æ˜“ç­–ç•¥å­¦ä¹ 

è¯¥æ¨¡å—å®ç°äº†å®Œæ•´çš„Actor-Criticæ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š
1. Actorç½‘ç»œï¼šç­–ç•¥ç½‘ç»œï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
2. Criticç½‘ç»œï¼šä»·å€¼ç½‘ç»œï¼Œè¯„ä¼°çŠ¶æ€ä»·å€¼
3. PPOç®—æ³•ï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œç¨³å®šä¸”é«˜æ•ˆçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•
4. ç»éªŒå›æ”¾ï¼šæ”¯æŒæ‰¹é‡è®­ç»ƒå’Œæ ·æœ¬æ•ˆç‡æå‡

ç‰¹ç‚¹ï¼š
- æ”¯æŒç¦»æ•£å’Œè¿ç»­åŠ¨ä½œç©ºé—´
- å†…ç½®æ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆDropout, BatchNorm, æƒé‡è¡°å‡ï¼‰
- è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦
- æ¢¯åº¦è£å‰ªå’Œè®­ç»ƒç¨³å®šæ€§ä¿è¯

Author: AI Assistant
Date: 2024
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from collections import deque, namedtuple
import logging
import math

from .mdp_environment import MDPState, MDPAction

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç»éªŒå…ƒç»„å®šä¹‰
Experience = namedtuple('Experience', [
    'state', 'action', 'reward', 'next_state', 'done', 'log_prob', 'value'
])


def _get_optimal_device(force_cpu: bool = True) -> str:
    """
    æ™ºèƒ½è®¾å¤‡é€‰æ‹© - ä¼˜åŒ–ä¸ºCPUå¤šçº¿ç¨‹
    
    Args:
        force_cpu: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨CPU (é»˜è®¤Trueï¼Œå› ä¸ºCPUå¤šçº¿ç¨‹æ›´é€‚åˆæ­¤ä»»åŠ¡)
    
    Returns:
        æœ€ä¼˜è®¾å¤‡å­—ç¬¦ä¸²
    """
    if force_cpu:
        print("ğŸ”§ å¼ºåˆ¶ä½¿ç”¨CPUè®¡ç®— + å¤šçº¿ç¨‹åŠ é€Ÿ (4è¿›ç¨‹)")
        return 'cpu'
    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
        print("ğŸš€ æ£€æµ‹åˆ°Apple Silicon GPUï¼Œä½¿ç”¨MPSåŠ é€Ÿ")
        return 'mps'
    elif torch.cuda.is_available():
        print(f"ğŸš€ æ£€æµ‹åˆ°CUDA GPU: {torch.cuda.get_device_name()}")
        return 'cuda'
    else:
        print("âš ï¸  ä½¿ç”¨CPUï¼Œå»ºè®®ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ")
        return 'cpu'


@dataclass
class AgentConfig:
    """Agenté…ç½®å‚æ•°"""
    # ç½‘ç»œæ¶æ„
    hidden_dim: int = 256
    num_layers: int = 3  # æ·»åŠ ç¼ºå¤±çš„ç½‘ç»œå±‚æ•°é…ç½®
    dropout_rate: float = 0.2
    use_batch_norm: bool = True
    
    # è®­ç»ƒè¶…å‚æ•°
    learning_rate: float = 3e-4
    batch_size: int = 64
    ppo_epochs: int = 10
    clip_epsilon: float = 0.2
    value_loss_coef: float = 0.5
    entropy_coef: float = 0.01
    max_grad_norm: float = 0.5  # æ·»åŠ ç¼ºå¤±çš„æ¢¯åº¦è£å‰ªé…ç½®
    
    # GAEå‚æ•°
    gamma: float = 0.99
    gae_lambda: float = 0.95
    
    # æ­£åˆ™åŒ–
    weight_decay: float = 1e-4
    target_kl: float = 0.01
    
    # è®¾å¤‡é…ç½® - æ”¯æŒMPSåŠ é€Ÿ  
    device: str = None  # å°†åœ¨__post_init__ä¸­è®¾ç½®
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†ï¼Œè®¾ç½®æœ€ä¼˜è®¾å¤‡"""
        if self.device is None:
            self.device = _get_optimal_device()


class ActorNetwork(nn.Module):
    """Actorç½‘ç»œ - ç­–ç•¥ç½‘ç»œ
    
    è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
    """
    
    def __init__(self, state_dim: int, action_dim: int, config: AgentConfig):
        super(ActorNetwork, self).__init__()
        self.config = config
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        input_dim = state_dim
        
        for i in range(config.num_layers):
            layers.append(nn.Linear(input_dim, config.hidden_dim))
            
            if config.use_batch_norm:
                layers.append(nn.BatchNorm1d(config.hidden_dim))
            
            layers.append(nn.ReLU())
            
            if config.dropout_rate > 0:
                layers.append(nn.Dropout(config.dropout_rate))
            
            input_dim = config.hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(config.hidden_dim, action_dim))
        
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        logits = self.network(state)
        return F.softmax(logits, dim=-1)
    
    def get_action_and_log_prob(self, state: torch.Tensor) -> Tuple[int, torch.Tensor, torch.Tensor]:
        """è·å–åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡"""
        probs = self.forward(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        entropy = dist.entropy()
        return action.item(), log_prob, entropy


class CriticNetwork(nn.Module):
    """Criticç½‘ç»œ - ä»·å€¼ç½‘ç»œ
    
    è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºçŠ¶æ€ä»·å€¼ä¼°è®¡
    """
    
    def __init__(self, state_dim: int, config: AgentConfig):
        super(CriticNetwork, self).__init__()
        self.config = config
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        input_dim = state_dim
        
        for i in range(config.num_layers):
            layers.append(nn.Linear(input_dim, config.hidden_dim))
            
            if config.use_batch_norm:
                layers.append(nn.BatchNorm1d(config.hidden_dim))
            
            layers.append(nn.ReLU())
            
            if config.dropout_rate > 0:
                layers.append(nn.Dropout(config.dropout_rate))
            
            input_dim = config.hidden_dim
        
        # è¾“å‡ºå±‚ï¼ˆå•ä¸€ä»·å€¼ï¼‰
        layers.append(nn.Linear(config.hidden_dim, 1))
        
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.network(state).squeeze(-1)


class ExperienceBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº
    
    å­˜å‚¨Agentä¸ç¯å¢ƒäº¤äº’çš„ç»éªŒï¼Œæ”¯æŒGAEä¼˜åŠ¿è®¡ç®—
    """
    
    def __init__(self, capacity: int = 10000):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
    
    def add(self, experience: Experience):
        """æ·»åŠ ç»éªŒ"""
        self.buffer.append(experience)
    
    def get_batch(self, batch_size: int) -> List[Experience]:
        """è·å–æ‰¹é‡ç»éªŒ"""
        if len(self.buffer) < batch_size:
            return list(self.buffer)
        
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        return [self.buffer[i] for i in indices]
    
    def get_all(self) -> List[Experience]:
        """è·å–æ‰€æœ‰ç»éªŒ"""
        return list(self.buffer)
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.buffer.clear()
    
    def __len__(self):
        return len(self.buffer)


class ActorCriticAgent:
    """Actor-Criticå¼ºåŒ–å­¦ä¹ Agent
    
    åŸºäºPPOç®—æ³•çš„å®Œæ•´RL Agentï¼ŒåŒ…æ‹¬ï¼š
    1. åŒç½‘ç»œæ¶æ„ï¼ˆActor + Criticï¼‰
    2. PPOè®­ç»ƒç®—æ³•
    3. GAEä¼˜åŠ¿è®¡ç®—
    4. è‡ªé€‚åº”å­¦ä¹ å’Œæ­£åˆ™åŒ–
    """
    
    def __init__(self, state_dim: int, action_dim: int, config: Optional[AgentConfig] = None):
        """
        Args:
            state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
            config: Agenté…ç½®å‚æ•°
        """
        self.config = config or AgentConfig()
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = ActorNetwork(state_dim, action_dim, self.config).to(self.config.device)
        self.critic = CriticNetwork(state_dim, self.config).to(self.config.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(
            self.actor.parameters(), 
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        self.critic_optimizer = optim.Adam(
            self.critic.parameters(), 
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_scheduler = optim.lr_scheduler.StepLR(
            self.actor_optimizer, step_size=1000, gamma=0.95
        )
        self.critic_scheduler = optim.lr_scheduler.StepLR(
            self.critic_optimizer, step_size=1000, gamma=0.95
        )
        
        # ç»éªŒç¼“å†²åŒº
        self.experience_buffer = ExperienceBuffer()
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'total_steps': 0,
            'total_episodes': 0,
            'actor_losses': [],
            'critic_losses': [],
            'rewards': [],
            'entropies': []
        }
        
        logger.info(f"åˆå§‹åŒ–Actor-Critic Agent: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
    
    def select_action(self, state: MDPState, training: bool = True) -> Tuple[MDPAction, Dict[str, Any]]:
        """é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
            info: é¢å¤–ä¿¡æ¯ï¼ˆæ¦‚ç‡ã€ä»·å€¼ç­‰ï¼‰
        """
        state_tensor = state.to_tensor(self.config.device).unsqueeze(0)
        
        # ä¿å­˜åŸå§‹è®­ç»ƒçŠ¶æ€
        actor_training = self.actor.training
        critic_training = self.critic.training
        
        try:
            # ä¸´æ—¶è®¾ä¸ºevalæ¨¡å¼ä»¥é¿å…BatchNormé—®é¢˜ï¼ˆbatch_size=1ï¼‰
            self.actor.eval()
            self.critic.eval()
            
            with torch.no_grad() if not training else torch.enable_grad():
                # è·å–åŠ¨ä½œæ¦‚ç‡
                if training:
                    action_idx, log_prob, entropy = self.actor.get_action_and_log_prob(state_tensor)
                    value = self.critic(state_tensor)
                else:
                    probs = self.actor(state_tensor)
                    action_idx = torch.argmax(probs, dim=-1).item()
                    log_prob = torch.log(probs[0, action_idx])
                    value = self.critic(state_tensor)
                    entropy = torch.tensor(0.0)
        finally:
            # æ¢å¤åŸå§‹è®­ç»ƒçŠ¶æ€
            if actor_training:
                self.actor.train()
            if critic_training:
                self.critic.train()
        
        # è½¬æ¢ä¸ºMDPåŠ¨ä½œ
        action = MDPAction.from_discrete_action(action_idx)
        
        info = {
            'action_idx': action_idx,
            'log_prob': log_prob.item() if training else log_prob.item(),
            'value': value.item(),
            'entropy': entropy.item() if training else 0.0
        }
        
        return action, info
    
    def store_experience(self, state: MDPState, action: int, reward: float, 
                        next_state: MDPState, done: bool, log_prob: float, value: float):
        """å­˜å‚¨ç»éªŒ"""
        experience = Experience(
            state=state.to_tensor(self.config.device),
            action=action,
            reward=reward,
            next_state=next_state.to_tensor(self.config.device),
            done=done,
            log_prob=log_prob,
            value=value
        )
        self.experience_buffer.add(experience)
    
    def compute_gae(self, experiences: List[Experience]) -> Tuple[torch.Tensor, torch.Tensor]:
        """è®¡ç®—GAEä¼˜åŠ¿å’Œå›æŠ¥
        
        GAE (Generalized Advantage Estimation) æä¾›äº†æ–¹å·®å’Œåå·®ä¹‹é—´çš„å¹³è¡¡
        """
        rewards = torch.tensor([exp.reward for exp in experiences], dtype=torch.float32)
        values = torch.tensor([exp.value for exp in experiences], dtype=torch.float32)
        dones = torch.tensor([exp.done for exp in experiences], dtype=torch.bool)
        
        # è®¡ç®—TD error
        next_values = torch.zeros_like(values)
        next_values[:-1] = values[1:]
        
        deltas = rewards + self.config.gamma * next_values * (~dones) - values
        
        # è®¡ç®—GAE
        advantages = torch.zeros_like(rewards)
        advantage = 0
        
        for t in reversed(range(len(experiences))):
            advantage = deltas[t] + self.config.gamma * self.config.gae_lambda * advantage * (~dones[t])
            advantages[t] = advantage
        
        # è®¡ç®—å›æŠ¥
        returns = advantages + values
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return advantages, returns
    
    def update_policy(self) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥ï¼ˆPPOç®—æ³•ï¼‰"""
        if len(self.experience_buffer) < self.config.batch_size:
            return {}
        
        # è·å–æ‰€æœ‰ç»éªŒ
        experiences = self.experience_buffer.get_all()
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        advantages, returns = self.compute_gae(experiences)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        states = torch.stack([exp.state for exp in experiences]).to(self.config.device)
        actions = torch.tensor([exp.action for exp in experiences], dtype=torch.long).to(self.config.device)
        old_log_probs = torch.tensor([exp.log_prob for exp in experiences], dtype=torch.float32).to(self.config.device)
        advantages = advantages.to(self.config.device)
        returns = returns.to(self.config.device)
        
        # è®­ç»ƒç»Ÿè®¡
        total_actor_loss = 0
        total_critic_loss = 0
        total_entropy = 0
        
        # PPOæ›´æ–°å¾ªç¯
        for epoch in range(self.config.ppo_epochs):
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = torch.randperm(len(experiences))
            
            for start_idx in range(0, len(experiences), self.config.batch_size):
                end_idx = min(start_idx + self.config.batch_size, len(experiences))
                batch_indices = indices[start_idx:end_idx]
                
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = returns[batch_indices]
                
                # å‰å‘ä¼ æ’­
                action_probs = self.actor(batch_states)
                values = self.critic(batch_states)
                
                # è®¡ç®—æ–°çš„å¯¹æ•°æ¦‚ç‡å’Œç†µ
                dist = Categorical(action_probs)
                new_log_probs = dist.log_prob(batch_actions)
                entropy = dist.entropy().mean()
                
                # è®¡ç®—æ¦‚ç‡æ¯”
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                
                # PPOè£å‰ªç›®æ ‡
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * batch_advantages
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # CriticæŸå¤±
                critic_loss = F.mse_loss(values, batch_returns)
                
                # æ€»æŸå¤±
                total_loss = actor_loss + self.config.value_loss_coef * critic_loss - self.config.entropy_coef * entropy
                
                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.max_grad_norm)
                
                self.actor_optimizer.step()
                self.critic_optimizer.step()
                
                total_actor_loss += actor_loss.item()
                total_critic_loss += critic_loss.item()
                total_entropy += entropy.item()
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.actor_scheduler.step()
        self.critic_scheduler.step()
        
        # æ¸…ç©ºç»éªŒç¼“å†²åŒº
        self.experience_buffer.clear()
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        avg_actor_loss = total_actor_loss / (self.config.ppo_epochs * len(experiences) // self.config.batch_size)
        avg_critic_loss = total_critic_loss / (self.config.ppo_epochs * len(experiences) // self.config.batch_size)
        avg_entropy = total_entropy / (self.config.ppo_epochs * len(experiences) // self.config.batch_size)
        
        self.training_stats['actor_losses'].append(avg_actor_loss)
        self.training_stats['critic_losses'].append(avg_critic_loss)
        self.training_stats['entropies'].append(avg_entropy)
        
        return {
            'actor_loss': avg_actor_loss,
            'critic_loss': avg_critic_loss,
            'entropy': avg_entropy,
            'learning_rate': self.actor_optimizer.param_groups[0]['lr']
        }
    
    def save_models(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'config': self.config,
            'training_stats': self.training_stats
        }, path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load_models(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.config.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        
        if 'training_stats' in checkpoint:
            self.training_stats = checkpoint['training_stats']
        
        logger.info(f"æ¨¡å‹å·²åŠ è½½: {path}")
    
    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        stats = self.training_stats.copy()
        
        if len(stats['actor_losses']) > 0:
            stats['avg_actor_loss'] = np.mean(stats['actor_losses'][-100:])
            stats['avg_critic_loss'] = np.mean(stats['critic_losses'][-100:])
            stats['avg_entropy'] = np.mean(stats['entropies'][-100:])
        
        return stats
    
    def set_training_mode(self, training: bool):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        self.actor.train(training)
        self.critic.train(training)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    print("æµ‹è¯•Actor-Critic Agent...")
    
    # åˆ›å»ºé…ç½®
    config = AgentConfig(
        hidden_dim=128,
        num_layers=2,
        learning_rate=1e-3
    )
    
    # åˆ›å»ºAgent
    state_dim = 10
    action_dim = 5
    agent = ActorCriticAgent(state_dim, action_dim, config)
    
    # åˆ›å»ºæ¨¡æ‹ŸçŠ¶æ€
    from .mdp_environment import MDPState
    import numpy as np
    
    mock_state = MDPState(
        market_features=np.random.randn(5).astype(np.float32),
        signal_confidence=0.7,
        signal_direction=1,
        current_position=0.0,
        unrealized_pnl=0.0,
        portfolio_value=100000.0,
        days_in_position=0,
        recent_volatility=0.02,
        current_price=100.0
    )
    
    # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
    action, info = agent.select_action(mock_state)
    print(f"é€‰æ‹©åŠ¨ä½œ: {action.target_position}")
    print(f"åŠ¨ä½œä¿¡æ¯: {info}")
    
    # æµ‹è¯•å­˜å‚¨ç»éªŒ
    agent.store_experience(
        state=mock_state,
        action=action.to_discrete(),
        reward=0.1,
        next_state=mock_state,
        done=False,
        log_prob=info['log_prob'],
        value=info['value']
    )
    
    print(f"ç»éªŒç¼“å†²åŒºå¤§å°: {len(agent.experience_buffer)}")
    print("Actor-Critic Agentæµ‹è¯•å®Œæˆï¼") 