#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®¡é“ - å®Œæ•´çš„RLäº¤æ˜“ç­–ç•¥è®­ç»ƒç³»ç»Ÿ

è¯¥æ¨¡å—å®ç°äº†ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
2. MDPç¯å¢ƒé…ç½®å’Œåˆå§‹åŒ–
3. Actor-Critic Agentè®­ç»ƒ
4. ç¨³å¥å›æµ‹å’Œæ€§èƒ½è¯„ä¼°
5. æ¨¡å‹ä¿å­˜å’Œéƒ¨ç½²å‡†å¤‡

æ ¸å¿ƒåŠŸèƒ½ï¼š
- åˆ†é˜¶æ®µè®­ç»ƒï¼ˆé¢„è®­ç»ƒã€ä¸»è®­ç»ƒã€å¾®è°ƒï¼‰
- è¶…å‚æ•°ä¼˜åŒ–å’Œè‡ªåŠ¨è°ƒä¼˜
- å®æ—¶ç›‘æ§å’Œæ—©åœæœºåˆ¶
- å¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒæ”¯æŒ
- å®Œæ•´çš„å®éªŒè·Ÿè¸ªå’Œæ—¥å¿—è®°å½•

Author: AI Assistant
Date: 2024
"""

import os
import sys
import json
import pickle
import numpy as np
import pandas as pd
import torch
import torch.multiprocessing as mp
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from pathlib import Path
import warnings
from concurrent.futures import ProcessPoolExecutor
import optuna
from tqdm import tqdm

from .mdp_environment import TradingMDPEnvironment, MDPState, MDPAction
from .actor_critic_agent import ActorCriticAgent, AgentConfig
from .robust_backtester import RobustBacktester, BacktestConfig

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from strategy.training.enhanced_signal_generator import EnhancedSignalGenerator
from strategy.training.enhanced_transformer import EnhancedTimeSeriesTransformer, TransformerConfig
from config.settings import load_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RLTrainingConfig:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒé…ç½®"""
    # è®­ç»ƒé˜¶æ®µ
    num_pretraining_episodes: int = 50
    num_main_training_episodes: int = 200
    num_fine_tuning_episodes: int = 50
    
    # ç¯å¢ƒå‚æ•°
    initial_capital: float = 100000.0
    transaction_cost: float = 0.00075
    max_position: float = 1.0
    lookback_window: int = 60
    
    # è®­ç»ƒç­–ç•¥
    curriculum_learning: bool = True
    experience_replay: bool = True
    prioritized_replay: bool = False
    
    # è¶…å‚æ•°ä¼˜åŒ–
    use_hyperparameter_tuning: bool = False
    n_trials: int = 50
    pruning_enabled: bool = True
    
    # å¹¶è¡Œè®­ç»ƒ
    use_multiprocessing: bool = True  # å¯ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    n_workers: int = 4
    
    # ç›‘æ§å’Œä¿å­˜
    save_interval: int = 50
    eval_frequency: int = 25
    checkpoint_dir: str = "rl_checkpoints"
    log_dir: str = "rl_logs"
    
    # æ—©åœå’Œè°ƒåº¦
    early_stopping_patience: int = 20
    learning_rate_schedule: bool = True
    
    # å®éªŒè·Ÿè¸ª
    experiment_name: str = "rl_trading_experiment"
    tags: List[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = ["rl", "trading", "actor_critic"]


class ExperimentTracker:
    """å®éªŒè·Ÿè¸ªå™¨ - è®°å½•è®­ç»ƒè¿‡ç¨‹å’Œç»“æœ"""
    
    def __init__(self, config):
        self.config = config
        rl_config = config.reinforcement_learning
        self.experiment_dir = Path(rl_config.experiment_dir) / rl_config.experiment_name
        self.log_dir = Path(rl_config.log_dir)
        self.results_dir = Path(rl_config.results_dir)
        
        # åˆ›å»ºç›®å½•
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è®°å½•
        self.metrics = {}
        self.hyperparameters = {}
        self.artifacts = []
        
    def save_config(self):
        """ä¿å­˜å®éªŒé…ç½®"""
        config_path = self.experiment_dir / "config.json"
        with open(config_path, 'w') as f:
            json.dump(asdict(self.config), f, indent=2, default=str)
    
    def log_metrics(self, metrics: Dict[str, Any], step: int, phase: str = "train"):
        """è®°å½•æŒ‡æ ‡"""
        log_entry = {
            'step': step,
            'phase': phase,
            'timestamp': datetime.now().isoformat(),
            **metrics
        }
        self.metrics[step] = log_entry # ä½¿ç”¨å­—å…¸å­˜å‚¨ï¼Œæ–¹ä¾¿åç»­ä¿å­˜
        
        # å®æ—¶ä¿å­˜
        if len(self.metrics) % 10 == 0:
            self.save_metrics()
    
    def save_metrics(self):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        metrics_path = self.experiment_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(self.metrics, f, indent=2)
    
    def log_hyperparameters(self, hyperparameters: Dict[str, Any]):
        """è®°å½•è¶…å‚æ•°"""
        self.hyperparameters.update(hyperparameters)
        
        hyperparams_path = self.experiment_dir / "hyperparameters.json"
        with open(hyperparams_path, 'w') as f:
            json.dump(self.hyperparameters, f, indent=2, default=str)
    
    def save_artifact(self, artifact: Any, name: str, artifact_type: str = "model"):
        """ä¿å­˜è®­ç»ƒäº§ç‰©"""
        artifact_dir = self.experiment_dir / "artifacts"
        artifact_dir.mkdir(exist_ok=True)
        
        if artifact_type == "model":
            artifact_path = artifact_dir / f"{name}.pth"
            torch.save(artifact, artifact_path)
        elif artifact_type == "data":
            artifact_path = artifact_dir / f"{name}.pkl"
            with open(artifact_path, 'wb') as f:
                pickle.dump(artifact, f)
        else:
            artifact_path = artifact_dir / f"{name}.json"
            with open(artifact_path, 'w') as f:
                json.dump(artifact, f, indent=2, default=str)
        
        self.artifacts.append(str(artifact_path))
        logger.info(f"ä¿å­˜äº§ç‰©: {name} -> {artifact_path}")


class CurriculumLearning:
    """è¯¾ç¨‹å­¦ä¹ ç­–ç•¥
    
    é€æ­¥å¢åŠ è®­ç»ƒéš¾åº¦ï¼Œæé«˜å­¦ä¹ æ•ˆç‡
    """
    
    def __init__(self, stages: List[Dict[str, Any]]):
        self.current_stage = 0
        self.stages = stages
    
    def get_current_stage(self) -> Dict[str, Any]:
        """è·å–å½“å‰è®­ç»ƒé˜¶æ®µé…ç½®"""
        return self.stages[min(self.current_stage, len(self.stages) - 1)]
    
    def advance_stage(self, performance_metric: float, threshold: float = 0.6):
        """æ ¹æ®æ€§èƒ½å†³å®šæ˜¯å¦è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        if performance_metric > threshold and self.current_stage < len(self.stages) - 1:
            self.current_stage += 1
            logger.info(f"è¯¾ç¨‹å­¦ä¹ è¿›å…¥é˜¶æ®µ: {self.stages[self.current_stage]['name']}")
            return True
        return False


class HyperparameterOptimizer:
    """è¶…å‚æ•°ä¼˜åŒ–å™¨
    
    ä½¿ç”¨Optunaè¿›è¡Œè‡ªåŠ¨è¶…å‚æ•°æœç´¢
    """
    
    def __init__(self, config: RLTrainingConfig):
        self.config = config
        self.study = None
    
    def suggest_hyperparameters(self, trial: optuna.Trial) -> Dict[str, Any]:
        """å»ºè®®è¶…å‚æ•°ç»„åˆ"""
        hyperparameters = {
            # Agentå‚æ•°
            'hidden_dim': trial.suggest_categorical('hidden_dim', [128, 256, 512]),
            'num_layers': trial.suggest_int('num_layers', 2, 4),
            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),
            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),
            'clip_epsilon': trial.suggest_float('clip_epsilon', 0.1, 0.3),
            'entropy_coef': trial.suggest_float('entropy_coef', 0.001, 0.1, log=True),
            
            # ç¯å¢ƒå‚æ•°
            'transaction_cost': trial.suggest_float('transaction_cost', 0.0001, 0.001),
            'volatility_penalty': trial.suggest_float('volatility_penalty', 0.05, 0.2),
            
            # è®­ç»ƒå‚æ•°
            'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),
            'gamma': trial.suggest_float('gamma', 0.95, 0.999)
        }
        
        return hyperparameters
    
    def optimize(self, objective_function: Callable, n_trials: int = 50) -> Dict[str, Any]:
        """æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        self.study = optuna.create_study(
            direction='maximize',
            pruner=optuna.pruners.MedianPruner() if self.config.pruning_enabled else None
        )
        
        self.study.optimize(objective_function, n_trials=n_trials)
        
        return self.study.best_params


class RLTrainingPipeline:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®¡é“
    
    å®Œæ•´çš„ç«¯åˆ°ç«¯RLè®­ç»ƒç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š
    1. æ•°æ®é¢„å¤„ç†å’Œç¯å¢ƒè®¾ç½®
    2. Agentè®­ç»ƒå’Œä¼˜åŒ–
    3. æ€§èƒ½è¯„ä¼°å’Œæ¨¡å‹ä¿å­˜
    4. è¶…å‚æ•°ä¼˜åŒ–å’Œå®éªŒè·Ÿè¸ª
    """
    
    def __init__(self, config):
        self.config = config
        self.rl_config = config.reinforcement_learning
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(self.rl_config.random_seed)
        torch.manual_seed(self.rl_config.random_seed)
        
        # è®¾ç½®è®¾å¤‡ - æ”¯æŒMPSåŠ é€Ÿ
        self.device = self._get_optimal_device()
        print(f"ğŸ”§ å¼ºåŒ–å­¦ä¹ è®­ç»ƒè®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.environment = None
        self.agent = None
        self.backtester = None
        self.tracker = ExperimentTracker(self.config)
        
        # åˆå§‹åŒ–è®­ç»ƒå†å²è®°å½•
        self.training_history = []
        
        # åˆå§‹åŒ–æ—©åœç›¸å…³å±æ€§
        self.best_performance = float('-inf')
        self.patience_counter = 0
        # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ é˜¶æ®µé…ç½®
        self.curriculum_phases = self._create_curriculum_phases()
        # åˆå§‹åŒ–è¯¾ç¨‹å­¦ä¹ 
        self.curriculum = CurriculumLearning(self.curriculum_phases) if self.rl_config.curriculum_learning else None
    
    def _get_optimal_device(self) -> torch.device:
        """
        æ™ºèƒ½è®¾å¤‡é€‰æ‹© - ä¼˜åŒ–ä¸ºCPUå¤šçº¿ç¨‹
        
        Returns:
            æœ€ä¼˜PyTorchè®¾å¤‡å¯¹è±¡
        """
        print("ğŸ”§ å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ + å¤šçº¿ç¨‹åŠ é€Ÿ")
        return torch.device('cpu')
    
    def _create_curriculum_phases(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºè¯¾ç¨‹å­¦ä¹ é˜¶æ®µé…ç½®"""
        return [
            {"name": "é¢„è®­ç»ƒé˜¶æ®µ", "difficulty": 0.3, "volatility_factor": 0.5, "episodes": self.rl_config.pre_training_episodes},
            {"name": "ä¸»è®­ç»ƒé˜¶æ®µ", "difficulty": 0.6, "volatility_factor": 0.8, "episodes": self.rl_config.num_episodes},
            {"name": "å¾®è°ƒé˜¶æ®µ", "difficulty": 1.0, "volatility_factor": 1.0, "episodes": self.rl_config.fine_tuning_episodes}
        ]
    
    def prepare_data(self, data_path: str) -> pd.DataFrame:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info(f"åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
        
        if data_path.endswith('.parquet'):
            data = pd.read_parquet(data_path)
        elif data_path.endswith('.csv'):
            data = pd.read_csv(data_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_path}")
        
        # æ•°æ®éªŒè¯
        required_columns = ['close', 'volume']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
        
        # æ·»åŠ ä¿¡å·ç‰¹å¾ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'signal_confidence' not in data.columns:
            logger.info("ç”Ÿæˆæ¨¡æ‹Ÿä¿¡å·ç‰¹å¾...")
            # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„ä¿¡å·ç”Ÿæˆå™¨
            data['signal_confidence'] = np.random.uniform(0.4, 0.8, len(data))
            data['signal_direction'] = np.random.choice([-1, 0, 1], len(data))
        
        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ: {len(data)} è¡Œ, {len(data.columns)} åˆ—")
        return data
    
    def create_environment(self, data: pd.DataFrame, 
                          stage_config: Optional[Dict[str, Any]] = None) -> TradingMDPEnvironment:
        """åˆ›å»ºMDPäº¤æ˜“ç¯å¢ƒ"""
        env_config = {
            'initial_capital': self.rl_config.initial_cash,
            'transaction_cost': self.rl_config.transaction_cost_bps / 10000,  # è½¬æ¢bpsä¸ºå°æ•°
            'max_position': self.rl_config.max_position,
            'lookback_window': self.rl_config.lookback_window
        }
        
        # åº”ç”¨è¯¾ç¨‹å­¦ä¹ é…ç½®
        if stage_config:
            if 'volatility_scale' in stage_config:
                # è°ƒæ•´æ•°æ®çš„æ³¢åŠ¨æ€§
                if 'returns' in data.columns:
                    data = data.copy()
                    data['returns'] *= stage_config['volatility_scale']
        
        return TradingMDPEnvironment(data, **env_config)
    
    def create_agent(self, state_dim: int, action_dim: int, 
                    hyperparameters: Optional[Dict[str, Any]] = None) -> ActorCriticAgent:
        """åˆ›å»ºActor-Critic Agent"""
        # é»˜è®¤é…ç½®
        agent_config = AgentConfig()
        
        # åº”ç”¨è¶…å‚æ•°
        if hyperparameters:
            for key, value in hyperparameters.items():
                if hasattr(agent_config, key):
                    setattr(agent_config, key, value)
        
        return ActorCriticAgent(state_dim, action_dim, agent_config)
    
    def train_single_episode(self, agent: ActorCriticAgent, 
                           env: TradingMDPEnvironment, 
                           episode_idx: int) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªepisode"""
        state = env.reset()
        total_reward = 0
        step_count = 0
        episode_metrics = {
            'rewards': [],
            'actions': [],
            'values': [],
            'entropies': []
        }
        
        while True:
            # é€‰æ‹©åŠ¨ä½œ
            action, action_info = agent.select_action(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_experience(
                state=state,
                action=action.to_discrete(),
                reward=reward,
                next_state=next_state,
                done=done,
                log_prob=action_info['log_prob'],
                value=action_info['value']
            )
            
            # è®°å½•æŒ‡æ ‡
            episode_metrics['rewards'].append(reward)
            episode_metrics['actions'].append(action.target_position)
            episode_metrics['values'].append(action_info['value'])
            episode_metrics['entropies'].append(action_info['entropy'])
            
            total_reward += reward
            step_count += 1
            state = next_state
            
            if done:
                break
        
        # æ›´æ–°ç­–ç•¥
        update_info = agent.update_policy()
        
        # è®¡ç®—episodeç»Ÿè®¡
        episode_stats = {
            'episode': episode_idx,
            'total_reward': total_reward,
            'episode_length': step_count,
            'avg_reward': total_reward / step_count if step_count > 0 else 0,
            'avg_value': np.mean(episode_metrics['values']) if episode_metrics['values'] else 0,
            'avg_entropy': np.mean(episode_metrics['entropies']) if episode_metrics['entropies'] else 0,
            'position_variance': np.var(episode_metrics['actions']) if episode_metrics['actions'] else 0,
            **update_info
        }
        
        # ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡
        env_metrics = env.get_performance_metrics()
        episode_stats.update(env_metrics)
        
        return episode_stats
    
    def evaluate_agent(self, agent: ActorCriticAgent, 
                      eval_data: pd.DataFrame) -> Dict[str, Any]:
        """è¯„ä¼°Agentæ€§èƒ½"""
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        eval_env = self.create_environment(eval_data)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        agent.set_training_mode(False)
        
        # è¿è¡Œè¯„ä¼°
        state = eval_env.reset()
        total_reward = 0
        step_count = 0
        
        while True:
            action, _ = agent.select_action(state, training=False)
            next_state, reward, done, info = eval_env.step(action)
            
            total_reward += reward
            step_count += 1
            state = next_state
            
            if done:
                break
        
        # è·å–ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡
        env_metrics = eval_env.get_performance_metrics()
        
        eval_metrics = {
            'eval_total_reward': total_reward,
            'eval_avg_reward': total_reward / step_count if step_count > 0 else 0,
            'eval_steps': step_count,
            **{f'eval_{k}': v for k, v in env_metrics.items()}
        }
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        agent.set_training_mode(True)
        
        return eval_metrics
    
    def save_checkpoint(self, agent: ActorCriticAgent, episode: int, 
                       metrics: Dict[str, Any]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'episode': episode,
            'agent_state_dict': agent.actor.state_dict(),
            'critic_state_dict': agent.critic.state_dict(),
            'optimizer_state_dict': agent.actor_optimizer.state_dict(),
            'config': self.config,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        checkpoint_path = os.path.join(self.rl_config.checkpoint_dir, f"checkpoint_episode_{episode}.pth")
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if metrics.get('eval_sharpe_ratio', -np.inf) > self.best_performance:
            self.best_performance = metrics.get('eval_sharpe_ratio', -np.inf)
            best_model_path = os.path.join(self.rl_config.checkpoint_dir, "best_model.pth")
            torch.save(checkpoint, best_model_path)
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹: å¤æ™®æ¯”ç‡={self.best_performance:.4f}")
    
    def check_early_stopping(self, metrics: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        current_performance = metrics.get('eval_sharpe_ratio', -np.inf)
        
        if current_performance > self.best_performance:
            self.best_performance = current_performance
            self.patience_counter = 0
        else:
            self.patience_counter += 1
        
        if self.patience_counter >= self.rl_config.early_stopping_patience:
            logger.info(f"æ—©åœè§¦å‘: è¿ç»­{self.patience_counter}æ¬¡è¯„ä¼°æ— æ”¹å–„")
            return True
        
        return False
    
    def run_training(self, data_path: str, 
                    eval_data_path: Optional[str] = None,
                    hyperparameters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ...")
        
        # å‡†å¤‡æ•°æ®
        train_data = self.prepare_data(data_path)
        
        if eval_data_path:
            eval_data = self.prepare_data(eval_data_path)
        else:
            # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æœ€å20%ä½œä¸ºè¯„ä¼°æ•°æ®
            split_idx = int(len(train_data) * 0.8)
            eval_data = train_data.iloc[split_idx:].reset_index(drop=True)
            train_data = train_data.iloc[:split_idx].reset_index(drop=True)
        
        # åˆ›å»ºç¯å¢ƒå’ŒAgent
        initial_env = self.create_environment(train_data)
        state_dim = initial_env.get_state_size()
        action_dim = initial_env.get_action_space_size()
        
        agent = self.create_agent(state_dim, action_dim, hyperparameters)
        
        # è®°å½•è¶…å‚æ•°
        if hyperparameters:
            self.tracker.log_hyperparameters(hyperparameters)
        
        # è®­ç»ƒå¾ªç¯
        total_episodes = (self.rl_config.pre_training_episodes + 
                                                   self.rl_config.num_episodes + 
                          self.rl_config.fine_tuning_episodes)
        
        training_phases = [
            ("pretraining", self.rl_config.pre_training_episodes),
            ("main_training", self.rl_config.num_episodes),
            ("fine_tuning", self.rl_config.fine_tuning_episodes)
        ]
        
        episode_count = 0
        
        for phase_name, num_episodes in training_phases:
            logger.info(f"å¼€å§‹{phase_name}é˜¶æ®µ: {num_episodes} episodes")
            
            for episode in tqdm(range(num_episodes), desc=f"{phase_name}"):
                episode_count += 1
                
                # è¯¾ç¨‹å­¦ä¹ 
                if self.curriculum:
                    stage_config = self.curriculum.get_current_stage()
                    env = self.create_environment(train_data, stage_config)
                else:
                    env = self.create_environment(train_data)
                
                # è®­ç»ƒå•ä¸ªepisode
                episode_metrics = self.train_single_episode(agent, env, episode_count)
                
                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                self.tracker.log_metrics(episode_metrics, episode_count, phase_name)
                self.training_history.append(episode_metrics)
                
                # å®šæœŸè¯„ä¼°
                if episode_count % self.rl_config.eval_frequency == 0:
                    eval_metrics = self.evaluate_agent(agent, eval_data)
                    self.tracker.log_metrics(eval_metrics, episode_count, "eval")
                    
                    # è¯¾ç¨‹å­¦ä¹ è¿›å±•
                    if self.curriculum:
                        self.curriculum.advance_stage(
                            eval_metrics.get('eval_sharpe_ratio', 0)
                        )
                    
                    # æ£€æŸ¥æ—©åœ
                    if self.check_early_stopping(eval_metrics):
                        logger.info("æ—©åœè§¦å‘ï¼Œç»“æŸè®­ç»ƒ")
                        break
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if episode_count % self.rl_config.save_frequency == 0:
                        combined_metrics = {**episode_metrics, **eval_metrics}
                        self.save_checkpoint(agent, episode_count, combined_metrics)
                
                # æ‰“å°è¿›åº¦
                if episode_count % 25 == 0:
                    logger.info(f"Episode {episode_count}: "
                              f"Reward={episode_metrics['total_reward']:.4f}, "
                              f"Sharpe={episode_metrics.get('sharpe_ratio', 0):.4f}")
        
        # æœ€ç»ˆè¯„ä¼°
        final_eval_metrics = self.evaluate_agent(agent, eval_data)
        self.tracker.log_metrics(final_eval_metrics, episode_count, "final_eval")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(agent, episode_count, final_eval_metrics)
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.tracker.save_artifact(self.training_history, "training_history", "data")
        
        logger.info("å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ")
        
        return {
            'final_metrics': final_eval_metrics,
            'training_history': self.training_history,
            'best_performance': self.best_performance,
            'total_episodes': episode_count
        }
    
    def run_hyperparameter_optimization(self, data_path: str) -> Dict[str, Any]:
        """è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        if not self.optimizer:
            raise ValueError("è¶…å‚æ•°ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
        
        def objective(trial):
            hyperparameters = self.optimizer.suggest_hyperparameters(trial)
            
            # è¿è¡Œç®€åŒ–è®­ç»ƒ
            old_config = self.config
            self.config = RLTrainingConfig(
                num_pretraining_episodes=10,
                num_main_training_episodes=20,
                num_fine_tuning_episodes=5,
                eval_frequency=10
            )
            
            try:
                results = self.run_training(data_path, hyperparameters=hyperparameters)
                performance = results['final_metrics'].get('eval_sharpe_ratio', 0)
                
                # æŠ¥å‘Šä¸­é—´ç»“æœç”¨äºå‰ªæ
                trial.report(performance, self.config.num_main_training_episodes)
                
                if trial.should_prune():
                    raise optuna.TrialPruned()
                
                return performance
            
            except Exception as e:
                logger.error(f"Trialå¤±è´¥: {e}")
                return -np.inf
            
            finally:
                self.config = old_config
        
        # æ‰§è¡Œä¼˜åŒ–
        best_params = self.optimizer.optimize(objective, self.config.n_trials)
        
        # è®°å½•æœ€ä½³è¶…å‚æ•°
        self.tracker.log_hyperparameters({"best_hyperparameters": best_params})
        
        return best_params
    
    def run_robust_backtest(self, data_path: str, 
                           model_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œç¨³å¥å›æµ‹"""
        logger.info("å¼€å§‹ç¨³å¥å›æµ‹...")
        
        # å‡†å¤‡æ•°æ®
        data = self.prepare_data(data_path)
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæä¾›ï¼‰
        if model_path:
            checkpoint = torch.load(model_path)
            initial_env = self.create_environment(data)
            agent = self.create_agent(
                initial_env.get_state_size(), 
                initial_env.get_action_space_size()
            )
            agent.actor.load_state_dict(checkpoint['agent_state_dict'])
            agent.critic.load_state_dict(checkpoint['critic_state_dict'])
        else:
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(self.rl_config.checkpoint_dir, "best_model.pth")
            if os.path.exists(best_model_path):
                return self.run_robust_backtest(data_path, best_model_path)
            else:
                raise ValueError("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
        
        # é…ç½®å›æµ‹
        backtest_config = BacktestConfig(
            train_window_size=252 * 2,
            test_window_size=63,
            step_size=21,
            embargo_period=5
        )
        
        # åˆ›å»ºå›æµ‹å™¨
        backtester = RobustBacktester(backtest_config)
        
        # è¿è¡ŒRLå›æµ‹
        environment_config = {
            'initial_capital': self.rl_config.initial_cash,
            'transaction_cost': self.rl_config.transaction_cost_bps / 10000,  # è½¬æ¢bpsä¸ºå°æ•°
            'max_position': self.rl_config.max_position
        }
        
        backtest_results = backtester.run_rl_backtest(
            data, agent, environment_config
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report = backtester.generate_report()
        self.tracker.save_artifact(report, "backtest_report", "data")
        self.tracker.save_artifact(backtest_results, "backtest_results", "data")
        
        logger.info("ç¨³å¥å›æµ‹å®Œæˆ")
        return backtest_results


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    # åˆ›å»ºé…ç½®
    config = RLTrainingConfig(
        num_pretraining_episodes=20,
        num_main_training_episodes=100,
        num_fine_tuning_episodes=20,
        experiment_name="demo_rl_trading",
        use_hyperparameter_tuning=False,
        curriculum_learning=True
    )
    
    # åˆ›å»ºè®­ç»ƒç®¡é“
    pipeline = RLTrainingPipeline(config)
    
    # æ¨¡æ‹Ÿæ•°æ®è·¯å¾„ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®è·¯å¾„ï¼‰
    data_path = "processed_data/featured_data_reduced.parquet"
    
    try:
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_path):
            logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            logger.info("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            np.random.seed(42)
            n_samples = 2000
            
            returns = np.random.normal(0.0001, 0.02, n_samples)
            prices = 100 * np.exp(np.cumsum(returns))
            
            demo_data = pd.DataFrame({
                'close': prices,
                'volume': np.random.lognormal(10, 0.5, n_samples),
                'signal_confidence': np.random.uniform(0.4, 0.8, n_samples),
                'signal_direction': np.random.choice([-1, 0, 1], n_samples),
                'feature_1': np.random.randn(n_samples),
                'feature_2': np.random.randn(n_samples),
                'returns': returns
            })
            
            # ä¿å­˜æ¨¡æ‹Ÿæ•°æ®
            os.makedirs(os.path.dirname(data_path), exist_ok=True)
            demo_data.to_parquet(data_path)
        
        # è¿è¡Œè®­ç»ƒ
        if config.use_hyperparameter_tuning:
            logger.info("å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")
            best_params = pipeline.run_hyperparameter_optimization(data_path)
            logger.info(f"æœ€ä½³è¶…å‚æ•°: {best_params}")
            
            # ä½¿ç”¨æœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒ
            results = pipeline.run_training(data_path, hyperparameters=best_params)
        else:
            results = pipeline.run_training(data_path)
        
        # è¿è¡Œç¨³å¥å›æµ‹
        backtest_results = pipeline.run_robust_backtest(data_path)
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        logger.info("è®­ç»ƒå®Œæˆ!")
        logger.info(f"æœ€ç»ˆæ€§èƒ½: å¤æ™®æ¯”ç‡={results['final_metrics'].get('eval_sharpe_ratio', 0):.4f}")
        logger.info(f"è®­ç»ƒè½®æ•°: {results['total_episodes']}")
        logger.info(f"å›æµ‹ç»“æœ: {len(backtest_results.get('rl_fold_results', []))} æŠ˜éªŒè¯")
        
        return results, backtest_results
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    results, backtest_results = main() 