# -*- coding: utf-8 -*-
"""
å¢å¼ºä¿¡å·ç”Ÿæˆå™¨
æ•´åˆTBMå’Œå…ƒæ ‡ç­¾æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡ã€ä½å™ªå£°çš„äº¤æ˜“ä¿¡å·
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
import os
from typing import Dict, List, Tuple, Optional, Union, Any
from datetime import datetime

from strategy.training.transformer_model import TimeSeriesTransformer
from strategy.training.meta_labeling import MetaLabeler
from config.settings import ProjectConfig, load_config


class EnhancedSignalGenerator:
    """
    å¢å¼ºä¿¡å·ç”Ÿæˆå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. åŠ è½½è®­ç»ƒå¥½çš„ä¸»æ¨¡å‹å’Œå…ƒæ ‡ç­¾æ¨¡å‹
    2. ç”Ÿæˆé«˜è´¨é‡ã€é«˜ç½®ä¿¡åº¦çš„äº¤æ˜“ä¿¡å·
    3. æä¾›ä¿¡å·å¼ºåº¦ã€ç½®ä¿¡åº¦ã€é£é™©è¯„ä¼°
    4. æ”¯æŒå®æ—¶æ¨ç†å’Œæ‰¹é‡å¤„ç†
    """
    
    def __init__(self, 
                 config: ProjectConfig = None,
                 primary_model_path: str = None,
                 meta_model_path: str = None,
                 scaler_path: str = None):
        """
        åˆå§‹åŒ–å¢å¼ºä¿¡å·ç”Ÿæˆå™¨
        
        Args:
            config: é¡¹ç›®é…ç½®
            primary_model_path: ä¸»æ¨¡å‹è·¯å¾„
            meta_model_path: å…ƒæ ‡ç­¾æ¨¡å‹è·¯å¾„  
            scaler_path: æ ‡å‡†åŒ–å™¨è·¯å¾„
        """
        self.config = config or load_config()
        
        # è®¾å¤‡é…ç½®
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")
        
        # æ¨¡å‹è·¯å¾„
        self.primary_model_path = primary_model_path or self.config.model.model_save_path
        self.meta_model_path = meta_model_path or self.config.model.meta_model_save_path
        self.scaler_path = scaler_path or self.config.model.scaler_path
        
        # æ¨¡å‹ç»„ä»¶
        self.primary_model = None
        self.meta_labeler = None
        self.scaler = None
        self.is_loaded = False
        
        print(f"åˆå§‹åŒ–å¢å¼ºä¿¡å·ç”Ÿæˆå™¨:")
        print(f"  - è®¾å¤‡: {self.device}")
        print(f"  - ä¸»æ¨¡å‹è·¯å¾„: {self.primary_model_path}")
        print(f"  - å…ƒæ ‡ç­¾æ¨¡å‹è·¯å¾„: {self.meta_model_path}")
        
    def load_models(self, num_features: int = None):
        """
        åŠ è½½æ‰€æœ‰æ¨¡å‹ç»„ä»¶
        
        Args:
            num_features: ç‰¹å¾æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä»æ•°æ®ä¸­æ¨æ–­
        """
        print("\nåŠ è½½æ¨¡å‹ç»„ä»¶...")
        
        # 1. åŠ è½½æ ‡å‡†åŒ–å™¨
        if os.path.exists(self.scaler_path):
            with open(self.scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            print(f"âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {self.scaler_path}")
            
            # ä»æ ‡å‡†åŒ–å™¨æ¨æ–­ç‰¹å¾æ•°é‡
            if num_features is None and hasattr(self.scaler, 'n_features_in_'):
                num_features = self.scaler.n_features_in_
        else:
            print(f"âš ï¸ æ ‡å‡†åŒ–å™¨ä¸å­˜åœ¨: {self.scaler_path}")
        
        # 2. åŠ è½½ä¸»æ¨¡å‹
        if os.path.exists(self.primary_model_path):
            if num_features is None:
                raise ValueError("æ— æ³•ç¡®å®šç‰¹å¾æ•°é‡ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š")
            
            # åˆ›å»ºæ¨¡å‹æ¶æ„
            model_params = self.config.model.get_model_params()
            self.primary_model = TimeSeriesTransformer(
                num_features=num_features,
                **model_params
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            state_dict = torch.load(self.primary_model_path, map_location=self.device)
            self.primary_model.load_state_dict(state_dict)
            self.primary_model.eval()
            
            print(f"âœ… ä¸»æ¨¡å‹åŠ è½½æˆåŠŸ: {self.primary_model_path}")
            print(f"   - ç‰¹å¾æ•°é‡: {num_features}")
            print(f"   - æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.primary_model.parameters()):,}")
        else:
            print(f"âŒ ä¸»æ¨¡å‹ä¸å­˜åœ¨: {self.primary_model_path}")
            return False
        
        # 3. åŠ è½½å…ƒæ ‡ç­¾æ¨¡å‹
        if self.config.model.use_meta_labeling and os.path.exists(self.meta_model_path):
            self.meta_labeler = MetaLabeler()
            self.meta_labeler.load_meta_model(self.meta_model_path)
            self.meta_labeler.primary_model = self.primary_model
            
            print(f"âœ… å…ƒæ ‡ç­¾æ¨¡å‹åŠ è½½æˆåŠŸ: {self.meta_model_path}")
        elif self.config.model.use_meta_labeling:
            print(f"âš ï¸ å…ƒæ ‡ç­¾æ¨¡å‹ä¸å­˜åœ¨: {self.meta_model_path}")
        else:
            print("ğŸ“ å…ƒæ ‡ç­¾åŠŸèƒ½æœªå¯ç”¨")
        
        self.is_loaded = True
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
        return True
    
    def preprocess_features(self, features: np.ndarray) -> np.ndarray:
        """
        é¢„å¤„ç†ç‰¹å¾æ•°æ®
        
        Args:
            features: åŸå§‹ç‰¹å¾æ•°ç»„
            
        Returns:
            é¢„å¤„ç†åçš„ç‰¹å¾æ•°ç»„
        """
        # å¤„ç†NaNå’Œæ— ç©·å¤§å€¼
        features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # æ ‡å‡†åŒ–
        if self.scaler is not None:
            original_shape = features.shape
            if len(original_shape) == 3:  # (n_samples, seq_len, n_features)
                # é‡å¡‘ä¸º2Dè¿›è¡Œæ ‡å‡†åŒ–
                features_2d = features.reshape(-1, features.shape[-1])
                features_2d = self.scaler.transform(features_2d)
                features = features_2d.reshape(original_shape)
            else:  # 2D features
                features = self.scaler.transform(features)
        
        return features.astype(np.float32)
    
    def generate_primary_signals(self, features: np.ndarray) -> Dict[str, np.ndarray]:
        """
        ä½¿ç”¨ä¸»æ¨¡å‹ç”ŸæˆåŸºç¡€ä¿¡å·
        
        Args:
            features: é¢„å¤„ç†åçš„ç‰¹å¾æ•°ç»„
            
        Returns:
            ä¸»æ¨¡å‹ä¿¡å·ç»“æœ
        """
        if not self.is_loaded:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_models()")
        
        self.primary_model.eval()
        
        # æ‰¹é‡æ¨ç†
        batch_size = 256
        predictions = []
        probabilities = []
        
        with torch.no_grad():
            for i in range(0, len(features), batch_size):
                batch_features = features[i:i+batch_size]
                batch_tensor = torch.FloatTensor(batch_features).to(self.device)
                
                outputs = self.primary_model(batch_tensor)
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                
                predictions.extend(preds.cpu().numpy())
                probabilities.extend(probs.cpu().numpy())
        
        predictions = np.array(predictions)
        probabilities = np.array(probabilities)
        
        # ç”ŸæˆåŸºç¡€ä¿¡å·
        if probabilities.shape[1] == 3:  # ä¸‰åˆ†ç±»
            # P(ä¸Šæ¶¨) - P(ä¸‹è·Œ)
            raw_signals = probabilities[:, 2] - probabilities[:, 0]
        else:  # äºŒåˆ†ç±»
            raw_signals = probabilities[:, 1] - probabilities[:, 0]
        
        return {
            'predictions': predictions,
            'probabilities': probabilities,
            'raw_signals': raw_signals,
            'signal_strength': np.abs(raw_signals),
            'confidence_score': np.max(probabilities, axis=1)
        }
    
    def generate_enhanced_signals(self, features: np.ndarray) -> Dict[str, np.ndarray]:
        """
        ç”Ÿæˆå¢å¼ºçš„äº¤æ˜“ä¿¡å·ï¼ˆåŒ…å«å…ƒæ ‡ç­¾è¿‡æ»¤ï¼‰
        
        Args:
            features: åŸå§‹ç‰¹å¾æ•°ç»„
            
        Returns:
            å¢å¼ºä¿¡å·ç»“æœå­—å…¸
        """
        if not self.is_loaded:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_models()")
        
        print(f"ç”Ÿæˆå¢å¼ºä¿¡å·ï¼Œæ ·æœ¬æ•°: {len(features)}")
        
        # 1. é¢„å¤„ç†ç‰¹å¾
        processed_features = self.preprocess_features(features)
        
        # 2. ç”Ÿæˆä¸»æ¨¡å‹ä¿¡å·
        primary_results = self.generate_primary_signals(processed_features)
        
        # 3. å¦‚æœæœ‰å…ƒæ ‡ç­¾æ¨¡å‹ï¼Œè¿›è¡Œä¿¡å·å¢å¼º
        if self.meta_labeler is not None and self.meta_labeler.is_fitted:
            print("ä½¿ç”¨å…ƒæ ‡ç­¾æ¨¡å‹å¢å¼ºä¿¡å·...")
            
            # ç”Ÿæˆè¿‡æ»¤åçš„ä¿¡å·
            meta_results = self.meta_labeler.generate_filtered_signals(
                processed_features,
                device=str(self.device)
            )
            
            # åˆå¹¶ç»“æœ
            enhanced_results = {
                **primary_results,
                'meta_confidence': meta_results['confidence_probabilities'],
                'filtered_signals': meta_results['filtered_signals'],
                'high_confidence_signals': meta_results['high_confidence_signals'],
                'high_confidence_mask': meta_results['high_confidence_mask'],
                'statistics': meta_results['statistics']
            }
            
            print(f"ä¿¡å·å¢å¼ºå®Œæˆ:")
            print(f"  - é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {meta_results['statistics']['high_conf_signal_ratio']:.4f}")
            print(f"  - å¹³å‡ç½®ä¿¡åº¦: {meta_results['statistics']['avg_confidence']:.4f}")
            
        else:
            print("ä½¿ç”¨ä¸»æ¨¡å‹ä¿¡å·ï¼ˆæ— å…ƒæ ‡ç­¾å¢å¼ºï¼‰")
            enhanced_results = primary_results
        
        return enhanced_results
    
    def generate_trading_decisions(self, 
                                 signals: Dict[str, np.ndarray],
                                 signal_threshold: float = 0.1,
                                 confidence_threshold: float = 0.6) -> pd.DataFrame:
        """
        åŸºäºä¿¡å·ç”Ÿæˆäº¤æ˜“å†³ç­–
        
        Args:
            signals: ä¿¡å·ç»“æœå­—å…¸
            signal_threshold: ä¿¡å·é˜ˆå€¼
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            äº¤æ˜“å†³ç­–DataFrame
        """
        # ä½¿ç”¨å¢å¼ºä¿¡å·è¿˜æ˜¯åŸå§‹ä¿¡å·
        if 'filtered_signals' in signals:
            main_signals = signals['filtered_signals']
            confidence_scores = signals['meta_confidence']
        else:
            main_signals = signals['raw_signals']
            confidence_scores = signals['confidence_score']
        
        # ç”Ÿæˆäº¤æ˜“å†³ç­–
        decisions = []
        
        for i in range(len(main_signals)):
            signal = main_signals[i]
            confidence = confidence_scores[i]
            
            # åŸºäºä¿¡å·å¼ºåº¦å’Œç½®ä¿¡åº¦åšå†³ç­–
            if abs(signal) >= signal_threshold and confidence >= confidence_threshold:
                if signal > 0:
                    action = 'BUY'
                    direction = 1
                else:
                    action = 'SELL'
                    direction = -1
            else:
                action = 'HOLD'
                direction = 0
            
            # è®¡ç®—ä»“ä½å¤§å°ï¼ˆåŸºäºä¿¡å·å¼ºåº¦å’Œç½®ä¿¡åº¦ï¼‰
            position_size = min(abs(signal) * confidence, 1.0) if action != 'HOLD' else 0.0
            
            decisions.append({
                'index': i,
                'signal': signal,
                'confidence': confidence,
                'action': action,
                'direction': direction,
                'position_size': position_size,
                'signal_strength': abs(signal),
                'meets_threshold': abs(signal) >= signal_threshold,
                'high_confidence': confidence >= confidence_threshold
            })
        
        df_decisions = pd.DataFrame(decisions)
        
        # ç»Ÿè®¡ä¿¡æ¯
        action_counts = df_decisions['action'].value_counts()
        high_conf_count = df_decisions['high_confidence'].sum()
        
        print(f"\näº¤æ˜“å†³ç­–ç»Ÿè®¡:")
        print(f"  - ä¹°å…¥ä¿¡å·: {action_counts.get('BUY', 0)}")
        print(f"  - å–å‡ºä¿¡å·: {action_counts.get('SELL', 0)}")
        print(f"  - æŒæœ‰ä¿¡å·: {action_counts.get('HOLD', 0)}")
        print(f"  - é«˜ç½®ä¿¡åº¦å†³ç­–: {high_conf_count} ({high_conf_count/len(df_decisions)*100:.2f}%)")
        print(f"  - å¹³å‡ä¿¡å·å¼ºåº¦: {df_decisions['signal_strength'].mean():.4f}")
        print(f"  - å¹³å‡ç½®ä¿¡åº¦: {df_decisions['confidence'].mean():.4f}")
        
        return df_decisions
    
    def batch_inference(self,
                       data: Union[pd.DataFrame, np.ndarray],
                       feature_columns: List[str] = None,
                       sequence_length: int = None) -> Dict[str, Any]:
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            data: è¾“å…¥æ•°æ®ï¼ˆDataFrameæˆ–numpyæ•°ç»„ï¼‰
            feature_columns: ç‰¹å¾åˆ—åï¼ˆå¦‚æœdataæ˜¯DataFrameï¼‰
            sequence_length: åºåˆ—é•¿åº¦
            
        Returns:
            æ¨ç†ç»“æœ
        """
        if not self.is_loaded:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_models()")
        
        sequence_length = sequence_length or self.config.model.sequence_length
        
        # å‡†å¤‡ç‰¹å¾æ•°æ®
        if isinstance(data, pd.DataFrame):
            if feature_columns is None:
                # è‡ªåŠ¨æ£€æµ‹ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ—¶é—´å’Œç›®æ ‡åˆ—ï¼‰
                exclude_cols = ['start_time', 'end_time', 'target', 'future_return']
                exclude_cols.extend([col for col in data.columns if 'target' in col or 'time' in col])
                feature_columns = [col for col in data.columns if col not in exclude_cols]
            
            # æ„å»ºåºåˆ—ç‰¹å¾
            features = []
            valid_indices = []
            
            for i in range(sequence_length, len(data)):
                try:
                    feature_sequence = data.iloc[i-sequence_length:i][feature_columns].values
                    if feature_sequence.shape[0] == sequence_length:
                        features.append(feature_sequence)
                        valid_indices.append(i)
                except:
                    continue
            
            features = np.array(features)
            
        else:  # numpyæ•°ç»„
            features = data
            valid_indices = list(range(len(features)))
        
        print(f"æ‰¹é‡æ¨ç†: {len(features)} ä¸ªæ ·æœ¬")
        
        # ç”Ÿæˆä¿¡å·
        signals = self.generate_enhanced_signals(features)
        
        # ç”Ÿæˆäº¤æ˜“å†³ç­–
        decisions = self.generate_trading_decisions(signals)
        decisions['original_index'] = valid_indices
        
        return {
            'signals': signals,
            'decisions': decisions,
            'feature_count': features.shape[-1] if len(features) > 0 else 0,
            'sample_count': len(features),
            'timestamp': datetime.now().isoformat()
        }
    
    def real_time_inference(self, 
                           latest_features: np.ndarray) -> Dict[str, Any]:
        """
        å®æ—¶æ¨ç†ï¼ˆå•ä¸ªæ ·æœ¬ï¼‰
        
        Args:
            latest_features: æœ€æ–°çš„ç‰¹å¾åºåˆ— (seq_len, n_features)
            
        Returns:
            å®æ—¶æ¨ç†ç»“æœ
        """
        if not self.is_loaded:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_models()")
        
        # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®
        if len(latest_features.shape) == 2:
            latest_features = latest_features.reshape(1, *latest_features.shape)
        
        # ç”Ÿæˆä¿¡å·
        signals = self.generate_enhanced_signals(latest_features)
        
        # æå–å•ä¸ªæ ·æœ¬çš„ç»“æœ
        result = {}
        for key, value in signals.items():
            if isinstance(value, np.ndarray) and len(value) > 0:
                result[key] = value[0] if value.ndim > 0 else value
            else:
                result[key] = value
        
        # ç”Ÿæˆäº¤æ˜“å†³ç­–
        decision = self.generate_trading_decisions(signals).iloc[0].to_dict()
        
        return {
            'signal': result,
            'decision': decision,
            'timestamp': datetime.now().isoformat()
        }
    
    def save_inference_results(self, 
                              results: Dict[str, Any], 
                              save_path: str):
        """
        ä¿å­˜æ¨ç†ç»“æœ
        
        Args:
            results: æ¨ç†ç»“æœ
            save_path: ä¿å­˜è·¯å¾„
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ä¿å­˜å†³ç­–DataFrame
        if 'decisions' in results:
            decisions_path = save_path.replace('.pkl', '_decisions.csv')
            results['decisions'].to_csv(decisions_path, index=False)
            print(f"äº¤æ˜“å†³ç­–å·²ä¿å­˜: {decisions_path}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        with open(save_path, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"æ¨ç†ç»“æœå·²ä¿å­˜: {save_path}")


def load_enhanced_signal_generator(config: ProjectConfig = None) -> EnhancedSignalGenerator:
    """
    ä¾¿åˆ©å‡½æ•°ï¼šåŠ è½½é¢„è®­ç»ƒçš„å¢å¼ºä¿¡å·ç”Ÿæˆå™¨
    
    Args:
        config: é¡¹ç›®é…ç½®
        
    Returns:
        åŠ è½½å¥½çš„EnhancedSignalGenerator
    """
    generator = EnhancedSignalGenerator(config)
    
    # å°è¯•è‡ªåŠ¨åŠ è½½æ¨¡å‹
    try:
        generator.load_models()
        print("âœ… å¢å¼ºä¿¡å·ç”Ÿæˆå™¨åŠ è½½æˆåŠŸ")
        return generator
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨å¹¶æ­£ç¡®è®­ç»ƒ")
        raise 