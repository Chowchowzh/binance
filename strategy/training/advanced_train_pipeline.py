# -*- coding: utf-8 -*-
"""
é«˜çº§è®­ç»ƒæµæ°´çº¿
æ•´åˆä¸‰åˆ†ç±»æ ‡ç­¾æ³•(TBM)å’Œå…ƒæ ‡ç­¾æŠ€æœ¯çš„å®Œæ•´ä¸¤é˜¶æ®µå­¦ä¹ æ¡†æ¶
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import os
import sys
import pickle
from datetime import datetime
from typing import Dict, Tuple, Any, Optional
import warnings

# Add project root to Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from data_processing.features.triple_barrier_labeling import TripleBarrierLabeler
from strategy.training.meta_labeling import MetaLabeler
from strategy.training.transformer_model import TimeSeriesTransformer
from config.settings import ProjectConfig, load_config
from data_processing.preprocessor import DataPreprocessor

warnings.filterwarnings('ignore')


class AdvancedTrainingPipeline:
    """
    é«˜çº§è®­ç»ƒæµæ°´çº¿
    
    å®Œæ•´çš„ä¸¤é˜¶æ®µå­¦ä¹ æ¡†æ¶ï¼š
    1. TBMæ ‡ç­¾ç”Ÿæˆï¼šä½¿ç”¨åŠ¨æ€è¾¹ç•Œç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾
    2. ä¸»æ¨¡å‹è®­ç»ƒï¼šTransformeræ¨¡å‹è¿½æ±‚é«˜å¬å›ç‡
    3. å…ƒæ ‡ç­¾è®­ç»ƒï¼šæ¬¡çº§æ¨¡å‹æå‡ç²¾åº¦ï¼Œè¿‡æ»¤å‡é˜³æ€§
    """
    
    def __init__(self, config: ProjectConfig = None):
        """
        åˆå§‹åŒ–é«˜çº§è®­ç»ƒæµæ°´çº¿
        
        Args:
            config: é¡¹ç›®é…ç½®
        """
        self.config = config or load_config()
        
        # è®¾å¤‡é…ç½®
        # å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œå¤šçº¿ç¨‹è®­ç»ƒ
        self.device = torch.device("cpu")
        print("ğŸ”§ é«˜çº§è®­ç»ƒç®¡é“ä½¿ç”¨CPU + å¤šçº¿ç¨‹åŠ é€Ÿ")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tbm_labeler = None
        self.primary_model = None
        self.meta_labeler = None
        self.preprocessor = None
        self.scaler = None
        
        # æ•°æ®ç¼“å­˜
        self.processed_data = None
        self.tbm_labels = None
        self.training_features = None
        self.training_labels = None
        
        print(f"åˆå§‹åŒ–é«˜çº§è®­ç»ƒæµæ°´çº¿:")
        print(f"  - è®¾å¤‡: {self.device}")
        print(f"  - TBMå¯ç”¨: True")
        print(f"  - å…ƒæ ‡ç­¾å¯ç”¨: {self.config.model.use_meta_labeling}")
        
    def load_and_prepare_data(self) -> pd.DataFrame:
        """
        åŠ è½½å’Œå‡†å¤‡æ•°æ®
        
        Returns:
            å¤„ç†åçš„æ•°æ®DataFrame
        """
        print("\n" + "=" * 60)
        print("ğŸ“Š æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
        print("=" * 60)
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        data_path = self.config.model.data_path
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        print(f"åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
        df = pd.read_parquet(data_path)
        print(f"åŸå§‹æ•°æ®ç»´åº¦: {df.shape}")
        
        # 2. åˆå§‹åŒ–é¢„å¤„ç†å™¨
        self.preprocessor = DataPreprocessor(config_path='config/config.json')
        
        # 3. åŠ è½½æˆ–åˆ›å»ºscaler
        scaler_path = self.config.model.scaler_path
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            print(f"å·²åŠ è½½æ ‡å‡†åŒ–å™¨: {scaler_path}")
        else:
            print("æ ‡å‡†åŒ–å™¨ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨åˆ›å»ºæ–°çš„æ ‡å‡†åŒ–å™¨")
            from sklearn.preprocessing import StandardScaler
            self.scaler = StandardScaler()
            print("å·²åˆ›å»ºæ–°çš„StandardScaler")

        self.processed_data = df
        return df
    
    def generate_tbm_labels(self, price_column: str = None) -> pd.DataFrame:
        """
        ç”ŸæˆTBMæ ‡ç­¾
        
        Args:
            price_column: ä»·æ ¼åˆ—å
            
        Returns:
            TBMæ ‡ç­¾DataFrame
        """
        print("\n" + "=" * 60)
        print("ğŸ¯ ä¸‰åˆ†ç±»æ ‡ç­¾æ³• (TBM) æ ‡ç­¾ç”Ÿæˆ")
        print("=" * 60)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_path = self.config.model.tbm_labels_cache_path
        if os.path.exists(cache_path):
            print(f"ä»ç¼“å­˜åŠ è½½TBMæ ‡ç­¾: {cache_path}")
            self.tbm_labels = pd.read_parquet(cache_path)
            return self.tbm_labels
        
        # ç¡®å®šä»·æ ¼åˆ—
        if price_column is None:
            # è‡ªåŠ¨æ£€æµ‹ä»·æ ¼åˆ—
            possible_columns = ['close', 'ETHUSDT_close', f'{self.config.data_collection.target_symbol}_close']
            price_column = None
            for col in possible_columns:
                if col in self.processed_data.columns:
                    price_column = col
                    break
            
            if price_column is None:
                raise ValueError("æ— æ³•æ‰¾åˆ°ä»·æ ¼åˆ—ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š")
        
        print(f"ä½¿ç”¨ä»·æ ¼åˆ—: {price_column}")
        
        # åˆå§‹åŒ–TBMæ ‡ç­¾å™¨
        tbm_params = self.config.model.get_tbm_params()
        self.tbm_labeler = TripleBarrierLabeler(**tbm_params)
        
        # æå–ä»·æ ¼åºåˆ—
        prices = self.processed_data[price_column].copy()
        
        # ç”Ÿæˆäº‹ä»¶è§¦å‘ç‚¹
        if self.config.model.tbm_use_cusum_events:
            print("ä½¿ç”¨CUSUMè¿‡æ»¤å™¨ç”Ÿæˆäº‹ä»¶...")
            event_indices = self.tbm_labeler.generate_cusum_events(
                prices, 
                threshold=self.config.model.tbm_cusum_threshold
            )
        else:
            event_indices = None  # ä½¿ç”¨é»˜è®¤ï¼šæ‰€æœ‰æœ‰æ•ˆç‚¹
        
        # ç”ŸæˆTBMæ ‡ç­¾
        tbm_labels = self.tbm_labeler.generate_triple_barrier_labels(
            prices=prices,
            event_indices=event_indices,
            volatility_method=self.config.model.tbm_volatility_method,
            n_jobs=1  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        # åˆ†ææ ‡ç­¾è´¨é‡
        quality_analysis = self.tbm_labeler.analyze_label_quality(tbm_labels)
        print(f"\næ ‡ç­¾è´¨é‡åˆ†æ:")
        print(f"  - æ€»äº‹ä»¶æ•°: {quality_analysis['total_events']}")
        print(f"  - å¹³å‡æŒä»“æœŸ: {quality_analysis['holding_period_stats']['mean']:.2f}")
        print(f"  - å¹³å‡æ”¶ç›Šç‡: {quality_analysis['return_stats']['mean']:.6f}")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        tbm_labels.to_parquet(cache_path)
        print(f"TBMæ ‡ç­¾å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_path}")
        
        self.tbm_labels = tbm_labels
        return tbm_labels
    
    def prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¢åŠ æ ·æœ¬æ•°é‡
        
        Returns:
            Tuple[features, labels]: è®­ç»ƒç‰¹å¾å’Œæ ‡ç­¾
        """
        print("\n" + "=" * 60)
        print("ğŸ”§ è®­ç»ƒæ•°æ®å‡†å¤‡")
        print("=" * 60)
        
        if self.processed_data is None:
            raise ValueError("å¤„ç†åçš„æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè°ƒç”¨load_and_prepare_data()")
        
        if self.tbm_labels is None:
            raise ValueError("TBMæ ‡ç­¾ä¸å­˜åœ¨ï¼Œè¯·å…ˆè°ƒç”¨generate_tbm_labels()")
        
        # 1. æ„å»ºç‰¹å¾-æ ‡ç­¾å¯¹åº”å…³ç³»
        feature_label_pairs = []
        
        # è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ—¶é—´å’Œç›®æ ‡åˆ—ï¼‰- æ›´ä¸¥æ ¼çš„è¿‡æ»¤
        exclude_cols = ['start_time', 'end_time', 'target', 'future_return', 'tbm_label', 'tbm_return_pct', 'tbm_holding_period', 'tbm_touch_type']
        
        # æ‰©å±•æ’é™¤åˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰æ—¶é—´ç›¸å…³åˆ—
        for col in self.processed_data.columns:
            if ('target' in col or 'time' in col.lower() or 'timestamp' in col.lower() or
                self.processed_data[col].dtype.name.startswith('datetime')):
                exclude_cols.append(col)
        
        feature_cols = [col for col in self.processed_data.columns if col not in exclude_cols]
        
        # å¼ºåˆ¶è½¬æ¢ç‰¹å¾åˆ—ä¸ºfloat32
        for col in feature_cols:
            if col in self.processed_data.columns:
                self.processed_data[col] = self.processed_data[col].astype(np.float32)
        
        print(f"ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)}")
        
        # 2. ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•æå–æ›´å¤šè®­ç»ƒæ ·æœ¬ - è§£å†³æ ·æœ¬æ•°é‡å°‘çš„é—®é¢˜
        sequence_length = self.config.model.sequence_length
        overlap_ratio = 0.5  # 50%é‡å ï¼Œå¢åŠ æ ·æœ¬æ•°é‡
        step_size = max(1, int(sequence_length * (1 - overlap_ratio)))
        
        print(f"ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•ï¼Œçª—å£å¤§å°: {sequence_length}, æ­¥é•¿: {step_size}")
        
        # é¦–å…ˆæ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„TBMäº‹ä»¶
        valid_events = []
        for _, tbm_row in self.tbm_labels.iterrows():
            event_idx = tbm_row['event_idx']
            label = tbm_row['label']
            
            # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦æœ‰æ•ˆ
            if pd.isna(label):
                continue
                
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
            if event_idx < sequence_length:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœªæ¥æ•°æ®ç”¨äºéªŒè¯
            if event_idx >= len(self.processed_data) - 1:
                continue
                
            valid_events.append((event_idx, label))
        
        print(f"æ‰¾åˆ° {len(valid_events)} ä¸ªæœ‰æ•ˆTBMäº‹ä»¶")
        
        # 3. ä¸ºæ¯ä¸ªæœ‰æ•ˆäº‹ä»¶ç”Ÿæˆå¤šä¸ªè®­ç»ƒæ ·æœ¬
        for event_idx, label in valid_events:
            # åœ¨äº‹ä»¶å‰çš„çª—å£å†…ç”Ÿæˆå¤šä¸ªæ ·æœ¬
            max_start_idx = event_idx - sequence_length
            min_start_idx = max(0, max_start_idx - sequence_length)
            
            # ç”Ÿæˆå¤šä¸ªèµ·å§‹ä½ç½®
            start_positions = list(range(min_start_idx, max_start_idx + 1, step_size))
            if len(start_positions) == 0:
                start_positions = [max_start_idx]
            
            for start_idx in start_positions:
                end_idx = start_idx + sequence_length
                
                # ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ
                if end_idx > len(self.processed_data):
                    continue
                
                try:
                    feature_sequence = self.processed_data.iloc[start_idx:end_idx][feature_cols].values
                    
                    # æ£€æŸ¥ç‰¹å¾åºåˆ—æ˜¯å¦å®Œæ•´
                    if feature_sequence.shape[0] != sequence_length:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤šçš„NaNå€¼
                    nan_ratio = np.isnan(feature_sequence).sum() / feature_sequence.size
                    if nan_ratio > 0.1:  # è¶…è¿‡10%çš„NaNå€¼å°±è·³è¿‡
                        continue
                    
                    # å¤„ç†NaNå€¼
                    feature_sequence = np.nan_to_num(feature_sequence, nan=0.0)
                    
                    feature_label_pairs.append((feature_sequence, label))
                    
                except Exception as e:
                    print(f"è­¦å‘Š: å¤„ç†åºåˆ— {start_idx}:{end_idx} æ—¶å‡ºé”™: {e}")
                    continue
        
        print(f"æˆåŠŸåˆ›å»º {len(feature_label_pairs)} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        # 4. è½¬æ¢ä¸ºnumpyæ•°ç»„
        if len(feature_label_pairs) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡å’ŒTBMæ ‡ç­¾")
        
        features = np.array([pair[0] for pair in feature_label_pairs])
        labels = np.array([pair[1] for pair in feature_label_pairs])
        
        # 5. åˆ›å»ºæˆ–æ›´æ–°æ ‡å‡†åŒ–å™¨
        if self.scaler is not None:
            # é‡å¡‘ä¸º2Dè¿›è¡Œæ ‡å‡†åŒ–
            original_shape = features.shape
            features_2d = features.reshape(-1, features.shape[-1])
            
            # å¦‚æœæ˜¯æ–°åˆ›å»ºçš„scalerï¼Œéœ€è¦å…ˆfit
            if not hasattr(self.scaler, 'scale_'):
                print("æ­£åœ¨æ‹Ÿåˆæ–°çš„æ ‡å‡†åŒ–å™¨...")
                features_2d = self.scaler.fit_transform(features_2d)
                
                # ä¿å­˜æ ‡å‡†åŒ–å™¨
                scaler_path = self.config.model.scaler_path
                os.makedirs(os.path.dirname(scaler_path), exist_ok=True)
                with open(scaler_path, 'wb') as f:
                    pickle.dump(self.scaler, f)
                print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜åˆ°: {scaler_path}")
            else:
                print("ä½¿ç”¨å·²æœ‰æ ‡å‡†åŒ–å™¨è¿›è¡Œè½¬æ¢...")
                features_2d = self.scaler.transform(features_2d)
            
            features = features_2d.reshape(original_shape)
            print("ç‰¹å¾å·²æ ‡å‡†åŒ–")
        
        # 6. è½¬æ¢æ ‡ç­¾æ ¼å¼ (TBM: -1,0,1 -> 0,1,2)
        labels = labels + 1
        
        print(f"æœ€ç»ˆè®­ç»ƒæ•°æ®:")
        print(f"  - ç‰¹å¾å½¢çŠ¶: {features.shape}")
        print(f"  - æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        print(f"  - æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels.astype(int))}")
        
        # å¼ºåˆ¶è½¬æ¢æ•°æ®ç±»å‹
        self.training_features = features.astype(np.float32)
        self.training_labels = labels.astype(np.int64)
        
        return self.training_features, self.training_labels
    
    def train_primary_model(self) -> Dict[str, Any]:
        """
        è®­ç»ƒä¸»æ¨¡å‹ (Transformer)
        
        Returns:
            è®­ç»ƒç»“æœç»Ÿè®¡
        """
        print("\n" + "=" * 60)
        print("ğŸ§  ä¸»æ¨¡å‹è®­ç»ƒ (ç¬¬ä¸€é˜¶æ®µ: é«˜å¬å›ç‡)")
        print("=" * 60)
        
        if self.training_features is None or self.training_labels is None:
            raise ValueError("è®­ç»ƒæ•°æ®æœªå‡†å¤‡ï¼Œè¯·å…ˆè°ƒç”¨prepare_training_data()")
        
        # 1. æ•°æ®é›†åˆ’åˆ†
        n_samples = len(self.training_features)
        split_idx = int(n_samples * self.config.model.train_test_split_ratio)
        
        train_features = self.training_features[:split_idx]
        train_labels = self.training_labels[:split_idx]
        val_features = self.training_features[split_idx:]
        val_labels = self.training_labels[split_idx:]
        
        print(f"æ•°æ®é›†åˆ’åˆ†:")
        print(f"  - è®­ç»ƒæ ·æœ¬: {len(train_features)}")
        print(f"  - éªŒè¯æ ·æœ¬: {len(val_features)}")
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(
            torch.FloatTensor(train_features),
            torch.LongTensor(train_labels)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(val_features),
            torch.LongTensor(val_labels)
        )
        
        batch_size = self.config.model.batch_size
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # 3. åˆå§‹åŒ–æ¨¡å‹
        num_features = train_features.shape[-1]
        model_params = self.config.model.get_model_params()
        
        self.primary_model = TimeSeriesTransformer(
            num_features=num_features,
            **model_params
        ).to(self.device)
        
        print(f"ä¸»æ¨¡å‹å‚æ•°:")
        total_params = sum(p.numel() for p in self.primary_model.parameters())
        print(f"  - æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"  - ç‰¹å¾ç»´åº¦: {num_features}")
        
        # 4. è®­ç»ƒè®¾ç½®
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(
            self.primary_model.parameters(), 
            lr=self.config.model.learning_rate
        )
        
        # 5. è®­ç»ƒå¾ªç¯
        best_val_accuracy = 0.0
        training_history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}
        
        for epoch in range(self.config.model.epochs):
            print(f"\nEpoch {epoch+1}/{self.config.model.epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            self.primary_model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_features, batch_labels in train_loader:
                batch_features = batch_features.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.primary_model(batch_features)
                loss = criterion(outputs, batch_labels)
                
                if not torch.isnan(loss):
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        self.primary_model.parameters(), 
                        self.config.model.clip_value
                    )
                    optimizer.step()
                    
                    train_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    train_total += batch_labels.size(0)
                    train_correct += (predicted == batch_labels).sum().item()
            
            avg_train_loss = train_loss / len(train_loader)
            train_accuracy = 100 * train_correct / train_total
            
            # éªŒè¯é˜¶æ®µ
            self.primary_model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch_features, batch_labels in val_loader:
                    batch_features = batch_features.to(self.device)
                    batch_labels = batch_labels.to(self.device)
                    
                    outputs = self.primary_model(batch_features)
                    loss = criterion(outputs, batch_labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += batch_labels.size(0)
                    val_correct += (predicted == batch_labels).sum().item()
            
            avg_val_loss = val_loss / len(val_loader)
            val_accuracy = 100 * val_correct / val_total
            
            # è®°å½•å†å²
            training_history['train_loss'].append(avg_train_loss)
            training_history['val_loss'].append(avg_val_loss)
            training_history['val_accuracy'].append(val_accuracy)
            
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}%")
            print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                model_save_path = self.config.model.model_save_path
                os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
                torch.save(self.primary_model.state_dict(), model_save_path)
                print(f"  ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {val_accuracy:.2f}%")
        
        print(f"\nä¸»æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"  - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
        print(f"  - æ¨¡å‹ä¿å­˜è·¯å¾„: {self.config.model.model_save_path}")
        
        return {
            'best_val_accuracy': best_val_accuracy,
            'training_history': training_history,
            'model_path': self.config.model.model_save_path
        }
    
    def train_meta_labeling(self) -> Dict[str, Any]:
        """
        è®­ç»ƒå…ƒæ ‡ç­¾æ¨¡å‹ (ç¬¬äºŒé˜¶æ®µ)
        
        Returns:
            å…ƒæ ‡ç­¾è®­ç»ƒç»“æœ
        """
        print("\n" + "=" * 60)
        print("ğŸ” å…ƒæ ‡ç­¾è®­ç»ƒ (ç¬¬äºŒé˜¶æ®µ: é«˜ç²¾åº¦)")
        print("=" * 60)
        
        if not self.config.model.use_meta_labeling:
            print("å…ƒæ ‡ç­¾åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡")
            return {}
        
        if self.primary_model is None:
            raise ValueError("ä¸»æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train_primary_model()")
        
        # 1. åˆå§‹åŒ–å…ƒæ ‡ç­¾å™¨
        meta_params = self.config.model.get_meta_labeling_params()
        self.meta_labeler = MetaLabeler(
            primary_model=self.primary_model,
            **meta_params
        )
        
        # 2. ä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒå…ƒæ ‡ç­¾æ¨¡å‹
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ï¼Œä½†ä¼šåœ¨å†…éƒ¨åˆ†å‰²
        train_features = self.training_features
        
        # å°†TBMæ ‡ç­¾è½¬æ¢å›åŸå§‹æ ¼å¼ (0,1,2 -> -1,0,1)
        true_labels = self.training_labels - 1
        
        # 3. ç”Ÿæˆä¸»æ¨¡å‹é¢„æµ‹
        primary_preds, primary_probs = self.meta_labeler.generate_primary_predictions(
            train_features, 
            device=str(self.device)
        )
        
        # 4. åˆ›å»ºæ¬¡çº§æ¨¡å‹ç‰¹å¾
        meta_features = self.meta_labeler.create_meta_features(
            train_features, 
            primary_preds, 
            primary_probs
        )
        
        # 5. åˆ›å»ºå…ƒæ ‡ç­¾
        meta_labels = self.meta_labeler.create_meta_labels(
            primary_preds, 
            true_labels,
            prediction_type=self.config.model.meta_prediction_type
        )
        
        # 6. è®­ç»ƒæ¬¡çº§æ¨¡å‹
        training_results = self.meta_labeler.fit_meta_model(
            meta_features, 
            meta_labels,
            validation_split=self.config.model.meta_validation_split
        )
        
        # 7. ä¿å­˜å…ƒæ ‡ç­¾æ¨¡å‹
        meta_model_path = self.config.model.meta_model_save_path
        self.meta_labeler.save_meta_model(meta_model_path)
        
        print(f"\nå…ƒæ ‡ç­¾è®­ç»ƒå®Œæˆ!")
        print(f"  - éªŒè¯AUC: {training_results.get('validation_auc', 0):.4f}")
        print(f"  - éªŒè¯ç²¾åº¦: {training_results.get('validation_precision', 0):.4f}")
        print(f"  - æ¨¡å‹ä¿å­˜è·¯å¾„: {meta_model_path}")
        
        return training_results
    
    def generate_enhanced_signals(self, test_features: np.ndarray = None) -> Dict[str, np.ndarray]:
        """
        ç”Ÿæˆå¢å¼ºçš„äº¤æ˜“ä¿¡å·
        
        Args:
            test_features: æµ‹è¯•ç‰¹å¾ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨éªŒè¯é›†
            
        Returns:
            ä¿¡å·ç»“æœå­—å…¸
        """
        print("\n" + "=" * 60)
        print("ğŸš€ ç”Ÿæˆå¢å¼ºäº¤æ˜“ä¿¡å·")
        print("=" * 60)
        
        if self.meta_labeler is None:
            print("å…ƒæ ‡ç­¾æ¨¡å‹æœªè®­ç»ƒï¼Œä½¿ç”¨ä¸»æ¨¡å‹ä¿¡å·")
            # TODO: å®ç°ä»…ä½¿ç”¨ä¸»æ¨¡å‹çš„ä¿¡å·ç”Ÿæˆ
            return {}
        
        # ä½¿ç”¨éªŒè¯é›†æ•°æ®
        if test_features is None:
            split_idx = int(len(self.training_features) * self.config.model.train_test_split_ratio)
            test_features = self.training_features[split_idx:]
        
        # ç”Ÿæˆè¿‡æ»¤åçš„ä¿¡å·
        signal_results = self.meta_labeler.generate_filtered_signals(
            test_features, 
            device=str(self.device)
        )
        
        print(f"ä¿¡å·ç”Ÿæˆå®Œæˆ!")
        print(f"  - åŸå§‹ä¿¡å·å¹³å‡å¼ºåº¦: {signal_results['statistics']['raw_signal_mean']:.4f}")
        print(f"  - è¿‡æ»¤ä¿¡å·å¹³å‡å¼ºåº¦: {signal_results['statistics']['filtered_signal_mean']:.4f}")
        print(f"  - é«˜ç½®ä¿¡åº¦ä¿¡å·æ¯”ä¾‹: {signal_results['statistics']['high_conf_signal_ratio']:.4f}")
        
        return signal_results
    
    def run_complete_pipeline(self, price_column: str = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµæ°´çº¿
        
        Args:
            price_column: ä»·æ ¼åˆ—å
            
        Returns:
            å®Œæ•´æµç¨‹ç»“æœ
        """
        print("ğŸš€ å¯åŠ¨é«˜çº§è®­ç»ƒæµæ°´çº¿")
        print("=" * 80)
        
        results = {}
        
        try:
            # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
            self.load_and_prepare_data()
            
            # 2. TBMæ ‡ç­¾ç”Ÿæˆ
            self.generate_tbm_labels(price_column)
            
            # 3. è®­ç»ƒæ•°æ®å‡†å¤‡
            self.prepare_training_data()
            
            # 4. ä¸»æ¨¡å‹è®­ç»ƒ
            primary_results = self.train_primary_model()
            results['primary_model'] = primary_results
            
            # 5. å…ƒæ ‡ç­¾è®­ç»ƒ
            if self.config.model.use_meta_labeling:
                meta_results = self.train_meta_labeling()
                results['meta_labeling'] = meta_results
            
            # 6. ä¿¡å·ç”Ÿæˆæµ‹è¯•
            signal_results = self.generate_enhanced_signals()
            results['signals'] = signal_results
            
            results['status'] = 'success'
            results['timestamp'] = datetime.now().isoformat()
            
            print("\n" + "=" * 80)
            print("ğŸ‰ é«˜çº§è®­ç»ƒæµæ°´çº¿å®Œæˆ!")
            print("=" * 80)
            print(f"âœ… ä¸»æ¨¡å‹æœ€ä½³å‡†ç¡®ç‡: {primary_results['best_val_accuracy']:.2f}%")
            
            if 'meta_labeling' in results:
                meta_auc = results['meta_labeling'].get('validation_auc', 0)
                print(f"âœ… å…ƒæ ‡ç­¾æ¨¡å‹AUC: {meta_auc:.4f}")
            
            if 'signals' in results:
                high_conf_ratio = results['signals']['statistics']['high_conf_signal_ratio']
                print(f"âœ… é«˜ç½®ä¿¡åº¦ä¿¡å·æ¯”ä¾‹: {high_conf_ratio:.4f}")
            
            print("=" * 80)
            
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒæµæ°´çº¿å¤±è´¥: {e}")
            results['status'] = 'failed'
            results['error'] = str(e)
            raise
        
        return results


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„é«˜çº§è®­ç»ƒæµæ°´çº¿"""
    try:
        # åŠ è½½é…ç½®
        config = load_config()
        
        # åˆ›å»ºè®­ç»ƒæµæ°´çº¿
        pipeline = AdvancedTrainingPipeline(config)
        
        # è¿è¡Œå®Œæ•´æµç¨‹
        results = pipeline.run_complete_pipeline()
        
        print("\nè®­ç»ƒå®Œæˆï¼å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–‡ä»¶è¿›è¡Œæ¨ç†ï¼š")
        print(f"  - ä¸»æ¨¡å‹: {config.model.model_save_path}")
        if config.model.use_meta_labeling:
            print(f"  - å…ƒæ ‡ç­¾æ¨¡å‹: {config.model.meta_model_save_path}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == '__main__':
    main() 