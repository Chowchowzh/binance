import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
import pandas as pd
import numpy as np
import os
import sys
import pickle
from datetime import datetime

# Add project root to Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from data_processing.torch_dataset import TimeSeriesDataset
from data_processing.preprocessor import DataPreprocessor
from strategy.training.transformer_model import TimeSeriesTransformer
from config.settings import ProjectConfig, load_config

def train_model(config: ProjectConfig = None):
    """
    Main function to orchestrate the model training process.
    """
    # Load configuration if not provided
    if config is None:
        config = load_config()
    
    print("=" * 80)
    print("ğŸš€ å¼€å§‹ Transformer æ¨¡å‹è®­ç»ƒ")
    print("=" * 80)
    print(f"â° è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # 1. Setup Device (MPS for Apple Silicon, CUDA for Nvidia, else CPU)
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # DataLoader num_workers should be 0 if using MPS
    num_workers = 0 if device.type == 'mps' else 4
    print(f"âš™ï¸  æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°: {num_workers}")

    # 2. åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨å¹¶åŠ è½½ scaler
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®é¢„å¤„ç†ä¸åŠ è½½")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
        preprocessor = DataPreprocessor(config_path='config/config.json')
        print("âœ… æ•°æ®é¢„å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åŠ è½½ scaler
        scaler_path = config.model.scaler_path
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½æ ‡å‡†åŒ–å™¨: {scaler_path}")
            print(f"   - æ ‡å‡†åŒ–å™¨ç±»å‹: {type(scaler).__name__}")
            if hasattr(scaler, 'n_features_in_'):
                print(f"   - ç‰¹å¾æ•°é‡: {scaler.n_features_in_}")
        else:
            print(f"âš ï¸  æ ‡å‡†åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}")
            print("   å°†ä½¿ç”¨æ•°æ®é›†ä¸­å·²æ ‡å‡†åŒ–çš„æ•°æ®")
            scaler = None
    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # 3. Load and Prepare Data
    data_path = config.model.data_path
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: \"{data_path}\"")
        print("   è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹å¤„ç†")
        return

    print(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
    
    try:
        # Create the training dataset instance using preprocessed data
        full_dataset = TimeSeriesDataset(
            data_path=data_path, 
            sequence_length=config.model.sequence_length,
            preprocessor=preprocessor
        )
        
        # Get number of features from the dataset
        num_features = full_dataset.features.shape[1]
        total_samples = len(full_dataset)
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"   - ç‰¹å¾æ•°é‡: {num_features}")
        print(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   - åºåˆ—é•¿åº¦: {config.model.sequence_length}")
        print(f"   - æ•°æ®å½¢çŠ¶: {full_dataset.features.shape}")
        
        # è¯¦ç»†æ£€æŸ¥ç›®æ ‡å‘é‡
        print(f"\nğŸ¯ ç›®æ ‡å‘é‡æ£€æŸ¥:")
        print(f"   - ç›®æ ‡æ•°æ®ç±»å‹: {full_dataset.targets.dtype}")
        print(f"   - ç›®æ ‡æ•°æ®å½¢çŠ¶: {full_dataset.targets.shape}")
        print(f"   - ç›®æ ‡å€¼èŒƒå›´: [{np.min(full_dataset.targets)}, {np.max(full_dataset.targets)}]")
        print(f"   - ç›®æ ‡å€¼æ ·ä¾‹ (å‰10ä¸ª): {full_dataset.targets[:10]}")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ NaN æˆ–æ— æ•ˆå€¼
        nan_count = np.isnan(full_dataset.targets).sum()
        inf_count = np.isinf(full_dataset.targets).sum()
        print(f"   - NaN å€¼æ•°é‡: {nan_count}")
        print(f"   - æ— ç©·å¤§å€¼æ•°é‡: {inf_count}")
        
        # æ£€æŸ¥ç›®æ ‡åˆ†å¸ƒ
        unique_targets, counts = np.unique(full_dataset.targets, return_counts=True)
        print(f"   - ç›®æ ‡ç±»åˆ«åˆ†å¸ƒ:")
        for target, count in zip(unique_targets, counts):
            percentage = (count / len(full_dataset.targets)) * 100
            print(f"     ç±»åˆ« {target}: {count} æ ·æœ¬ ({percentage:.2f}%)")
        
        # æ£€æŸ¥ç±»åˆ«æ•°é‡æ˜¯å¦åˆç†
        num_classes = len(unique_targets)
        print(f"   - æ€»ç±»åˆ«æ•°: {num_classes}")
        
        if num_classes < 2:
            print(f"   âŒ é”™è¯¯: åªæœ‰ {num_classes} ä¸ªç±»åˆ«ï¼Œæ— æ³•è¿›è¡Œåˆ†ç±»è®­ç»ƒ")
            print(f"   ğŸ’¡ å»ºè®®: æ£€æŸ¥ç›®æ ‡ç”Ÿæˆé€»è¾‘ï¼Œç¡®ä¿æœ‰å¤šä¸ªç±»åˆ«")
            return
        elif num_classes > 10:
            print(f"   âš ï¸  è­¦å‘Š: ç±»åˆ«æ•°é‡è¾ƒå¤š ({num_classes})ï¼Œè¯·ç¡®è®¤è¿™æ˜¯é¢„æœŸçš„")
        else:
            print(f"   âœ… ç±»åˆ«æ•°é‡åˆç†")
        
        # æ£€æŸ¥ç‰¹å¾å’Œæ ‡å‡†åŒ–å™¨ä¸€è‡´æ€§
        if scaler is not None:
            print(f"\nğŸ”§ æ ‡å‡†åŒ–å™¨ä¸€è‡´æ€§æ£€æŸ¥:")
            if hasattr(scaler, 'n_features_in_'):
                print(f"   - æ ‡å‡†åŒ–å™¨ç‰¹å¾æ•°: {scaler.n_features_in_}")
                print(f"   - æ•°æ®ç‰¹å¾æ•°: {num_features}")
                if scaler.n_features_in_ == num_features:
                    print(f"   - âœ… ç‰¹å¾ç»´åº¦å®Œå…¨åŒ¹é…")
                else:
                    print(f"   - âš ï¸  ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼Œå¯èƒ½å½±å“æ¨ç†ä¸€è‡´æ€§")
            
            # æ£€æŸ¥ç‰¹å¾æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
            print(f"   - ç‰¹å¾å€¼èŒƒå›´: [{np.min(full_dataset.features):.4f}, {np.max(full_dataset.features):.4f}]")
            print(f"   - ç‰¹å¾å€¼å‡å€¼: {np.mean(full_dataset.features):.4f}")
            print(f"   - ç‰¹å¾å€¼æ ‡å‡†å·®: {np.std(full_dataset.features):.4f}")
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ ‡å‡†åŒ–
            mean_close_to_zero = abs(np.mean(full_dataset.features)) < 0.1
            std_close_to_one = abs(np.std(full_dataset.features) - 1.0) < 0.2
            if mean_close_to_zero and std_close_to_one:
                print(f"   âœ… æ•°æ®ä¼¼ä¹å·²ç»æ ‡å‡†åŒ–ï¼ˆå‡å€¼â‰ˆ0ï¼Œæ ‡å‡†å·®â‰ˆ1ï¼‰")
            else:
                print(f"   âš ï¸  æ•°æ®å¯èƒ½æœªæ ‡å‡†åŒ–ï¼Œè¯·æ£€æŸ¥é¢„å¤„ç†æµç¨‹")
        
        print(f"   ğŸ¯ æ•°æ®æ£€æŸ¥å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒ")

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        return

    # 4. æ•°æ®é›†åˆ’åˆ†
    print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ† (è®­ç»ƒ/éªŒè¯æ¯”ä¾‹: {config.model.train_test_split_ratio:.2f})")
    train_size = int(len(full_dataset) * config.model.train_test_split_ratio)
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    training_params = config.model.get_training_params()
    model_params = config.model.get_model_params()
    
    print(f"   - è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"   - éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    
    train_loader = DataLoader(train_dataset, batch_size=training_params['batch_size'], shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=training_params['batch_size'], shuffle=False, num_workers=num_workers)
    
    print(f"   - è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"   - éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {training_params['batch_size']}")

    # 5. Initialize Model, Loss, and Optimizer
    print(f"\n" + "=" * 60)
    print("ğŸ§  æ¨¡å‹åˆå§‹åŒ–")
    print("=" * 60)
    
    model = TimeSeriesTransformer(
        num_features=num_features,
        **model_params
    ).to(device)

    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   - æ¨¡å‹å‚æ•°: {model_params}")
    print(f"   - æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'])
    
    print(f"   - æŸå¤±å‡½æ•°: {type(criterion).__name__}")
    print(f"   - ä¼˜åŒ–å™¨: {type(optimizer).__name__}")
    print(f"   - å­¦ä¹ ç‡: {training_params['learning_rate']}")

    # 6. Training Loop
    print(f"\n" + "=" * 60)
    print("ğŸ‹ï¸  å¼€å§‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    print(f"   - è®­ç»ƒè½®æ•°: {training_params['epochs']}")
    print(f"   - æ¢¯åº¦è£å‰ª: {training_params['clip_value']}")
    
    best_val_accuracy = 0.0
    training_start_time = datetime.now()

    for epoch in range(training_params['epochs']):
        epoch_start_time = datetime.now()
        print(f"\nğŸ”„ Epoch {epoch+1}/{training_params['epochs']} - {epoch_start_time.strftime('%H:%M:%S')}")
        print("-" * 50)
        
        # --- Training Phase ---
        model.train()
        total_train_loss = 0
        correct_train = 0
        total_train = 0
        
        print("ğŸ“ˆ è®­ç»ƒé˜¶æ®µ...")
        for i, (sequences, labels) in enumerate(train_loader):
            sequences, labels = sequences.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(sequences)
            loss = criterion(outputs, labels)
            
            # Check for nan loss before backpropagation
            if torch.isnan(loss):
                print(f"âš ï¸  æ£€æµ‹åˆ° NaN æŸå¤± - Epoch [{epoch+1}/{training_params['epochs']}], Step [{i+1}/{len(train_loader)}]. è·³è¿‡æ­¤æ‰¹æ¬¡.")
                continue

            loss.backward()
            
            # Gradient Clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), training_params['clip_value'])
            
            optimizer.step()

            total_train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
            
            if (i + 1) % 50 == 0:
                current_acc = 100 * correct_train / total_train if total_train > 0 else 0
                print(f"   Step [{i+1}/{len(train_loader)}] - Loss: {loss.item():.4f}, Acc: {current_acc:.2f}%")

        avg_train_loss = total_train_loss / len(train_loader)
        train_accuracy = 100 * correct_train / total_train

        # --- Validation Phase ---
        print("ğŸ“Š éªŒè¯é˜¶æ®µ...")
        model.eval()
        total_val_loss = 0
        correct_val = 0
        total_val = 0
        with torch.no_grad():
            for i, (sequences, labels) in enumerate(val_loader):
                sequences, labels = sequences.to(device), labels.to(device)
                outputs = model(sequences)
                loss = criterion(outputs, labels)
                total_val_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        avg_val_loss = total_val_loss / len(val_loader)
        val_accuracy = 100 * correct_val / total_val

        epoch_end_time = datetime.now()
        epoch_duration = epoch_end_time - epoch_start_time
        
        print(f"\nğŸ“‹ Epoch {epoch+1} ç»“æœæ€»ç»“:")
        print(f"   â±ï¸  è€—æ—¶: {epoch_duration}")
        print(f"   ğŸ“ˆ è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"   ğŸ“ˆ è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}%")
        print(f"   ğŸ“Š éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        print(f"   ğŸ“Š éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%")

        # --- Save Best Model ---
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            model_save_path = config.model.model_save_path
            
            # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            
            torch.save(model.state_dict(), model_save_path)
            print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")
            print(f"   ğŸ“ ä¿å­˜è·¯å¾„: {model_save_path}")
            print(f"   ğŸ¯ éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%")
        else:
            print(f"   å½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")

    training_end_time = datetime.now()
    total_training_time = training_end_time - training_start_time
    
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print(f"â° è®­ç»ƒç»“æŸæ—¶é—´: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é•¿: {total_training_time}")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹ä¿å­˜ä½ç½®: {config.model.model_save_path}")
    print(f"ğŸ“Š ä½¿ç”¨çš„æ ‡å‡†åŒ–å™¨: {config.model.scaler_path}")
    print("=" * 80)

if __name__ == '__main__':
    train_model() 